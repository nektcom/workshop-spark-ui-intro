{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Introduction to Spark UI\n",
    "## Learn to Monitor and Understand Your Spark Jobs\n",
    "\n",
    "**Duration:** 1 hour  \n",
    "**Level:** Complete beginner (no Spark knowledge required)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "1. Understand what Spark is doing when you run code\n",
    "2. Find and read the Spark UI dashboard\n",
    "3. Identify jobs, stages, and tasks\n",
    "4. Spot performance issues in your Spark applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Starting Spark\n",
    "\n",
    "Before we can do anything, we need to start Spark. Run the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/30 12:28:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is ready!\n",
      "Version: 3.5.0\n",
      "\n",
      "==================================================\n",
      "ACTION REQUIRED: Open the Spark UI\n",
      "==================================================\n",
      "\n",
      "Open this URL in your browser:\n",
      "\n",
      "  >>> http://localhost:4040 <<<\n",
      "\n",
      "You should see the Spark UI with these tabs:\n",
      "- Jobs      (we'll focus on this first)\n",
      "- Stages  \n",
      "- Storage\n",
      "- Environment\n",
      "- Executors\n",
      "- SQL\n",
      "\n",
      "Go to the JOBS tab now.\n",
      "It should be EMPTY - no jobs yet!\n",
      "\n",
      "Keep this browser tab open. We'll check it after each step.\n",
      "\n",
      "NOTE: We disabled AQE (Adaptive Query Execution) for this workshop\n",
      "to make the Spark UI easier to understand. One action = one job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.1: Start Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing Spark session\n",
    "if \"spark\" in globals():\n",
    "    globals()[\"spark\"].stop()\n",
    "\n",
    "# Create a new Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark-UI-Workshop\")\n",
    "    .master(\"spark://127.0.0.1:7077\")\n",
    "    .config(\"spark.ui.port\", \"4040\")\n",
    "    .config(\"spark.ui.host\", \"0.0.0.0\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"8\")\n",
    "    .config(\"spark.cores.max\", \"8\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark is ready!\")\n",
    "print(f\"Version: {spark.version}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ACTION REQUIRED: Open the Spark UI\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Open this URL in your browser:\n",
    "\n",
    "  >>> http://localhost:4040 <<<\n",
    "\n",
    "You should see the Spark UI with these tabs:\n",
    "- Jobs      (we'll focus on this first)\n",
    "- Stages  \n",
    "- Storage\n",
    "- Environment\n",
    "- Executors\n",
    "- SQL\n",
    "\n",
    "Go to the JOBS tab now.\n",
    "It should be EMPTY - no jobs yet!\n",
    "\n",
    "Keep this browser tab open. We'll check it after each step.\n",
    "\n",
    "NOTE: We disabled AQE (Adaptive Query Execution) for this workshop\n",
    "to make the Spark UI easier to understand. One action = one job!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Understanding Spark Architecture\n",
    "\n",
    "Before we start coding, let's understand how Spark works.\n",
    "\n",
    "---\n",
    "\n",
    "### The Components\n",
    "\n",
    "```\n",
    "    YOUR CODE (this notebook)\n",
    "           │\n",
    "           ▼\n",
    "    ┌─────────────────┐\n",
    "    │     DRIVER      │  ← Your \"control center\"\n",
    "    │                 │    - Runs your Python code\n",
    "    │  SparkSession   │    - Plans how to execute queries\n",
    "    │  SparkContext   │    - Coordinates the executors\n",
    "    └────────┬────────┘    - Collects results back to you\n",
    "             │\n",
    "             │ sends tasks\n",
    "             ▼\n",
    "    ┌─────────────────┐\n",
    "    │     MASTER      │  ← The \"manager\" (spark://127.0.0.1:7077)\n",
    "    │                 │    - Knows which workers are available\n",
    "    │  Cluster Mgr    │    - Assigns resources to applications\n",
    "    └────────┬────────┘\n",
    "             │\n",
    "             │ manages\n",
    "             ▼\n",
    "    ┌─────────────────────────────────────────────────────┐\n",
    "    │                    EXECUTORS                        │\n",
    "    │                                                     │\n",
    "    │  ┌───────────┐  ┌───────────┐  ┌───────────┐        │\n",
    "    │  │ Executor 1│  │ Executor 2│  │ Executor 3│  ...   │\n",
    "    │  │           │  │           │  │           │        │\n",
    "    │  │  [Task]   │  │  [Task]   │  │  [Task]   │        │\n",
    "    │  │  [Task]   │  │  [Task]   │  │  [Task]   │        │\n",
    "    │  └───────────┘  └───────────┘  └───────────┘        │\n",
    "    │                                                     │\n",
    "    │  ← The \"workers\" that actually process your data    │\n",
    "    │    - Each executor runs on a machine                │\n",
    "    │    - Has its own memory and CPU cores               │\n",
    "    │    - Executes tasks in parallel                     │\n",
    "    └─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**In OUR setup (Spark Standalone on Docker):**\n",
    "- Master: `spark://127.0.0.1:7077`\n",
    "- We have 1 executor with 8 cores and 512MB memory\n",
    "- Driver: This notebook (running locally)\n",
    "\n",
    "---\n",
    "\n",
    "### Jobs, Stages, and Tasks\n",
    "\n",
    "When you run code, Spark breaks it down into smaller pieces:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                           JOB                                   │\n",
    "│                                                                 │\n",
    "│   Created when you call an ACTION (show, count, collect, etc)   │\n",
    "│                                                                 │\n",
    "│   ┌─────────────────────────────────────────────────────────┐   │\n",
    "│   │                      STAGE 1                            │   │\n",
    "│   │                                                         │   │\n",
    "│   │   A group of operations that can run WITHOUT shuffling  │   │\n",
    "│   │                                                         │   │\n",
    "│   │   ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐          │   │\n",
    "│   │   │Task 1│ │Task 2│ │Task 3│ │Task 4│ │ ...  │          │   │\n",
    "│   │   └──────┘ └──────┘ └──────┘ └──────┘ └──────┘          │   │\n",
    "│   │                                                         │   │\n",
    "│   │   Tasks run IN PARALLEL on different data partitions    │   │\n",
    "│   └─────────────────────────────────────────────────────────┘   │\n",
    "│                           │                                     │\n",
    "│                    SHUFFLE (data exchange)                      │\n",
    "│                           │                                     │\n",
    "│                           ▼                                     │\n",
    "│   ┌─────────────────────────────────────────────────────────┐   │\n",
    "│   │                      STAGE 2                            │   │\n",
    "│   │                                                         │   │\n",
    "│   │   ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐          │   │\n",
    "│   │   │Task 1│ │Task 2│ │Task 3│ │Task 4│ │ ...  │          │   │\n",
    "│   │   └──────┘ └──────┘ └──────┘ └──────┘ └──────┘          │   │\n",
    "│   └─────────────────────────────────────────────────────────┘   │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Summary:**\n",
    "- **JOB** = The whole computation (triggered by an action)\n",
    "- **STAGE** = A piece of work that doesn't need data shuffling\n",
    "- **TASK** = The smallest unit of work (1 task = 1 partition)\n",
    "- **SHUFFLE** = Moving data between executors (expensive!)\n",
    "\n",
    "---\n",
    "\n",
    "### What Causes a New Stage?\n",
    "\n",
    "| Operation | Why it needs a shuffle |\n",
    "|-----------|------------------------|\n",
    "| `groupBy()` | Needs to group all rows with same key together |\n",
    "| `join()` | Needs to bring matching rows together |\n",
    "| `orderBy()` | Needs to sort across all data |\n",
    "| `repartition()` | Redistributes data across partitions |\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "df.filter(...).select(...).groupBy(...).count()\n",
    "```\n",
    "\n",
    "```\n",
    "STAGE 1                          STAGE 2\n",
    "┌─────────────────────────┐      ┌─────────────────────────┐\n",
    "│ filter                  │      │ groupBy (final combine) │\n",
    "│   ↓                     │      │   ↓                     │\n",
    "│ select                  │      │ count                   │\n",
    "│   ↓                     │      │                         │\n",
    "│ groupBy (partial agg)   │      │                         │\n",
    "│   ↓                     │      │                         │\n",
    "│ write shuffle files     │ ───► │ read shuffle files      │\n",
    "└─────────────────────────┘      └─────────────────────────┘\n",
    "                          SHUFFLE\n",
    "                       (data moves!)\n",
    "```\n",
    "\n",
    "The `groupBy` is split across stages:\n",
    "1. **Stage 1**: Does partial aggregation + writes shuffle files\n",
    "2. **Shuffle**: Data physically moves between executors\n",
    "3. **Stage 2**: Reads shuffled data + final aggregation\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Concept | What is it? | How many? |\n",
    "|---------|-------------|-----------|\n",
    "| **JOB** | Triggered by an action | 1 per action |\n",
    "| **STAGE** | Separated by shuffles | 1+ per job |\n",
    "| **TASK** | Processes one partition | 1 per partition |\n",
    "| **PARTITION** | A chunk of your data | Configurable |\n",
    "| **EXECUTOR** | A worker process | Depends on cluster |\n",
    "\n",
    "**We'll see ALL of this in the Spark UI as we go through the workshop!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Spark is LAZY\n",
    "\n",
    "This is the **most important concept** to understand about Spark.\n",
    "\n",
    "Let's see it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1: Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame created!\n",
      "\n",
      "==================================================\n",
      "NOW CHECK THE SPARK UI - JOBS TAB\n",
      "==================================================\n",
      "\n",
      "Go to http://localhost:4040 and look at the Jobs tab.\n",
      "\n",
      "Do you see any jobs?\n",
      "\n",
      "...\n",
      "\n",
      "NO! The Jobs tab is still EMPTY!\n",
      "\n",
      "WHY? Because Spark is LAZY.\n",
      "\n",
      "We told Spark to create a DataFrame, but Spark said:\n",
      "\"OK, I'll remember that. But I won't actually do anything\n",
      " until you ask me for results.\"\n",
      "\n",
      "This is called LAZY EVALUATION.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1: Create a DataFrame\n",
    "#\n",
    "# spark.range(10) creates numbers from 0 to 9\n",
    "# Think of it like a list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "df = spark.range(10)\n",
    "\n",
    "print(\"DataFrame created!\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NOW CHECK THE SPARK UI - JOBS TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Go to http://localhost:4040 and look at the Jobs tab.\n",
    "\n",
    "Do you see any jobs?\n",
    "\n",
    "...\n",
    "\n",
    "NO! The Jobs tab is still EMPTY!\n",
    "\n",
    "WHY? Because Spark is LAZY.\n",
    "\n",
    "We told Spark to create a DataFrame, but Spark said:\n",
    "\"OK, I'll remember that. But I won't actually do anything\n",
    " until you ask me for results.\"\n",
    "\n",
    "This is called LAZY EVALUATION.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concept: Lazy Evaluation\n",
    "\n",
    "Spark divides operations into two types:\n",
    "\n",
    "| Type | Examples | Creates a Job? |\n",
    "|------|----------|----------------|\n",
    "| **Transformations** | filter, select, groupBy, join | NO - Spark just remembers them |\n",
    "| **Actions** | show, count, collect, write | YES - Spark does the work! |\n",
    "\n",
    "Think of it like cooking:\n",
    "- **Transformations** = Writing down the recipe steps\n",
    "- **Actions** = Actually cooking and serving the dish\n",
    "\n",
    "Let's trigger an **action** to make Spark work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Trigger an action with .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "\n",
      "==================================================\n",
      "NOW CHECK THE SPARK UI - JOBS TAB\n",
      "==================================================\n",
      "\n",
      "Go back to http://localhost:4040 (Jobs tab)\n",
      "\n",
      "NOW you should see a job!\n",
      "\n",
      "Look for:\n",
      "- Description: \"showString at NativeMethodAccessorImpl.java:0\"\n",
      "- A BLUE bar (meaning: completed successfully)\n",
      "- Stages: 2/2\n",
      "- Tasks: shows how many parallel work units ran\n",
      "\n",
      "CONGRATULATIONS! You just ran your first Spark job!\n",
      "\n",
      "KEY INSIGHT:\n",
      "The job only appeared when we called .show() (an action).\n",
      "Creating the DataFrame (2.1) did NOT create a job.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2.2: Trigger an action with .show()\n",
    "#\n",
    "# .show() is an ACTION - it tells Spark \"give me results NOW!\"\n",
    "\n",
    "df.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NOW CHECK THE SPARK UI - JOBS TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Go back to http://localhost:4040 (Jobs tab)\n",
    "\n",
    "NOW you should see a job!\n",
    "\n",
    "Look for:\n",
    "- Description: \"showString at NativeMethodAccessorImpl.java:0\"\n",
    "- A BLUE bar (meaning: completed successfully)\n",
    "- Stages: 2/2\n",
    "- Tasks: shows how many parallel work units ran\n",
    "\n",
    "CONGRATULATIONS! You just ran your first Spark job!\n",
    "\n",
    "KEY INSIGHT:\n",
    "The job only appeared when we called .show() (an action).\n",
    "Creating the DataFrame (2.1) did NOT create a job.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Add job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n",
      "\n",
      "==================================================\n",
      "NOW CHECK THE SPARK UI - JOBS TAB\n",
      "==================================================\n",
      "\n",
      "Look for:\n",
      "- Description: \"2.3: My first df.show()\"\n",
      "\n",
      "KEY INSIGHT:\n",
      "Adding context make it easier to understand \n",
      "what is happening when jobs start to scale\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.3: Add job description\n",
    "#\n",
    "# Let's give this job a name so we can find it in easily Spark UI\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"2.3: My first df.show()\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NOW CHECK THE SPARK UI - JOBS TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Look for:\n",
    "- Description: \"2.3: My first df.show()\"\n",
    "\n",
    "KEY INSIGHT:\n",
    "Adding context make it easier to understand \n",
    "what is happening when jobs start to scale\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Actions Create Jobs\n",
    "\n",
    "Every time you call an **action**, Spark creates a new **job**.\n",
    "\n",
    "Let's run more actions and watch the Jobs tab grow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1: Another action - count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 10\n",
      "\n",
      "==================================================\n",
      "CHECK THE SPARK UI - JOBS TAB\n",
      "==================================================\n",
      "\n",
      "You should see MORE jobs now.\n",
      "\n",
      "KEY POINT: Each ACTION creates job(s) in Spark UI.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3.1: Another action - count()\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"3.1: Count the rows\")\n",
    "total = df.count()\n",
    "\n",
    "print(f\"Total rows: {total}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHECK THE SPARK UI - JOBS TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "You should see MORE jobs now.\n",
    "\n",
    "KEY POINT: Each ACTION creates job(s) in Spark UI.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Another action - first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: Row(id=0)\n",
      "\n",
      "==================================================\n",
      "CHECK THE SPARK UI - JOBS TAB\n",
      "==================================================\n",
      "\n",
      "Even more jobs now!\n",
      "\n",
      "SUMMARY SO FAR:\n",
      "- 2.1: Created DataFrame → NO job (lazy!)\n",
      "- 2.2: Called .show()    → Job appeared!\n",
      "- 3.1: Called .count()   → More job(s)!\n",
      "- 3.2: Called .first()   → More job(s)!\n",
      "\n",
      "Pattern: ACTIONS create JOBS. Transformations don't.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2: Another action - first()\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"3.2: Get first row\")\n",
    "\n",
    "first_row = df.first()\n",
    "print(f\"First row: {first_row}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHECK THE SPARK UI - JOBS TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Even more jobs now!\n",
    "\n",
    "SUMMARY SO FAR:\n",
    "- 2.1: Created DataFrame → NO job (lazy!)\n",
    "- 2.2: Called .show()    → Job appeared!\n",
    "- 3.1: Called .count()   → More job(s)!\n",
    "- 3.2: Called .first()   → More job(s)!\n",
    "\n",
    "Pattern: ACTIONS create JOBS. Transformations don't.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Understanding Adaptive Query Execution (AQE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WHAT IS ADAPTIVE QUERY EXECUTION (AQE)?\n",
      "============================================================\n",
      "\n",
      "AQE is enabled by default in Spark 3.0+. It optimizes queries\n",
      "at RUNTIME by gathering statistics as the query runs.\n",
      "\n",
      "In this workshop, we DISABLED AQE to keep things predictable:\n",
      "  spark.sql.adaptive.enabled = false\n",
      "\n",
      "Let's see what happens when AQE is ON vs OFF:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 2 (AQE ON):  Done! Check Spark UI.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 (AQE OFF): Done! Check Spark UI.\n",
      "\n",
      "============================================================\n",
      "CHECK THE SPARK UI - COMPARE THE TWO RUNS\n",
      "============================================================\n",
      "\n",
      "STEP 1: Compare task counts in the JOBS TAB\n",
      "===========================================\n",
      "Look at the \"Tasks\" column for each job:\n",
      "\n",
      "  \"3.3a: GroupBy with AQE OFF\" → ~200 tasks\n",
      "  \"3.3b: GroupBy with AQE ON\"  → fewer tasks!\n",
      "\n",
      "This is the partition coalescing in action!\n",
      "\n",
      "\n",
      "STEP 2: See the details in the SQL TAB\n",
      "======================================\n",
      "1. Go to the SQL tab\n",
      "2. Find the two queries (most recent at top)\n",
      "3. Click on each query to see the execution plan\n",
      "\n",
      "For \"AQE OFF\" query:\n",
      "  - Look for \"Exchange\" node (the shuffle)\n",
      "  - It will show the original partition count\n",
      "\n",
      "For \"AQE ON\" query:\n",
      "  - Look for \"AQEShuffleRead\" node\n",
      "  - This replaces the regular Exchange\n",
      "  - Click on it to see: \"number of partitions: X\"\n",
      "  - X will be much smaller than 200!\n",
      "\n",
      "\n",
      "STEP 3: Compare stage details in STAGES TAB\n",
      "===========================================\n",
      "1. Go to the Stages tab\n",
      "2. Find the shuffle stages (Stage with \"Shuffle Read\")\n",
      "3. Compare \"Tasks\" column:\n",
      "   - AQE OFF stage: ~200 tasks\n",
      "   - AQE ON stage: fewer tasks\n",
      "\n",
      "\n",
      "WHY IS AQE FASTER?\n",
      "==================\n",
      "Without AQE:\n",
      "  - Spark uses 200 shuffle partitions (the default)\n",
      "  - 200 small tasks = lots of scheduling overhead\n",
      "\n",
      "With AQE:\n",
      "  - After Stage 1, Spark measures actual data size\n",
      "  - Sees data is small → coalesces 200 → few partitions\n",
      "  - Fewer tasks = less overhead = faster!\n",
      "\n",
      "\n",
      "WHAT ELSE DOES AQE DO?\n",
      "======================\n",
      "1. COALESCES PARTITIONS - Combines small partitions (seen above!)\n",
      "2. OPTIMIZES JOINS - Switches to broadcast join if data is small\n",
      "3. HANDLES SKEW - Splits large partitions to balance work\n",
      "\n",
      "\n",
      "WHY WE DISABLED IT FOR THIS WORKSHOP:\n",
      "=====================================\n",
      "AQE makes Spark faster, but less predictable for learning:\n",
      "  - One action might create multiple jobs\n",
      "  - \"Skipped\" stages appear unexpectedly\n",
      "\n",
      "FOR PRODUCTION: Leave AQE ON (default)\n",
      "FOR LEARNING: Disable it to see exactly what Spark is doing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.3: (OPTIONAL) Understanding Adaptive Query Execution (AQE)\n",
    "#\n",
    "# You might notice that some actions create MULTIPLE jobs.\n",
    "# This is often caused by AQE (Adaptive Query Execution).\n",
    "\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WHAT IS ADAPTIVE QUERY EXECUTION (AQE)?\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "AQE is enabled by default in Spark 3.0+. It optimizes queries\n",
    "at RUNTIME by gathering statistics as the query runs.\n",
    "\n",
    "In this workshop, we DISABLED AQE to keep things predictable:\n",
    "  spark.sql.adaptive.enabled = false\n",
    "\n",
    "Let's see what happens when AQE is ON vs OFF:\n",
    "\"\"\")\n",
    "\n",
    "# Create test data (10 million rows, 10,000 categories)\n",
    "df = spark.range(10_000_000).withColumn(\"category\", (col(\"id\") % 10_000))\n",
    "\n",
    "# --- AQE ON ---\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.sparkContext.setJobDescription(\"3.3b: GroupBy with AQE ON\")\n",
    "df.groupBy(\"category\").agg(count(\"*\")).collect()\n",
    "print(\"Run 2 (AQE ON):  Done! Check Spark UI.\")\n",
    "\n",
    "# --- AQE OFF ---\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.sparkContext.setJobDescription(\"3.3a: GroupBy with AQE OFF\")\n",
    "df.groupBy(\"category\").agg(count(\"*\")).collect()\n",
    "print(\"Run 1 (AQE OFF): Done! Check Spark UI.\")\n",
    "\n",
    "\n",
    "# --- Reset ---\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECK THE SPARK UI - COMPARE THE TWO RUNS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "STEP 1: Compare task counts in the JOBS TAB\n",
    "===========================================\n",
    "Look at the \"Tasks\" column for each job:\n",
    "\n",
    "  \"3.3a: GroupBy with AQE OFF\" → ~200 tasks\n",
    "  \"3.3b: GroupBy with AQE ON\"  → fewer tasks!\n",
    "\n",
    "This is the partition coalescing in action!\n",
    "\n",
    "\n",
    "STEP 2: See the details in the SQL TAB\n",
    "======================================\n",
    "1. Go to the SQL tab\n",
    "2. Find the two queries (most recent at top)\n",
    "3. Click on each query to see the execution plan\n",
    "\n",
    "For \"AQE OFF\" query:\n",
    "  - Look for \"Exchange\" node (the shuffle)\n",
    "  - It will show the original partition count\n",
    "\n",
    "For \"AQE ON\" query:\n",
    "  - Look for \"AQEShuffleRead\" node\n",
    "  - This replaces the regular Exchange\n",
    "  - Click on it to see: \"number of partitions: X\"\n",
    "  - X will be much smaller than 200!\n",
    "\n",
    "\n",
    "STEP 3: Compare stage details in STAGES TAB\n",
    "===========================================\n",
    "1. Go to the Stages tab\n",
    "2. Find the shuffle stages (Stage with \"Shuffle Read\")\n",
    "3. Compare \"Tasks\" column:\n",
    "   - AQE OFF stage: ~200 tasks\n",
    "   - AQE ON stage: fewer tasks\n",
    "\n",
    "\n",
    "WHY IS AQE FASTER?\n",
    "==================\n",
    "Without AQE:\n",
    "  - Spark uses 200 shuffle partitions (the default)\n",
    "  - 200 small tasks = lots of scheduling overhead\n",
    "\n",
    "With AQE:\n",
    "  - After Stage 1, Spark measures actual data size\n",
    "  - Sees data is small → coalesces 200 → few partitions\n",
    "  - Fewer tasks = less overhead = faster!\n",
    "\n",
    "\n",
    "WHAT ELSE DOES AQE DO?\n",
    "======================\n",
    "1. COALESCES PARTITIONS - Combines small partitions (seen above!)\n",
    "2. OPTIMIZES JOINS - Switches to broadcast join if data is small\n",
    "3. HANDLES SKEW - Splits large partitions to balance work\n",
    "\n",
    "\n",
    "WHY WE DISABLED IT FOR THIS WORKSHOP:\n",
    "=====================================\n",
    "AQE makes Spark faster, but less predictable for learning:\n",
    "  - One action might create multiple jobs\n",
    "  - \"Skipped\" stages appear unexpectedly\n",
    "\n",
    "FOR PRODUCTION: Leave AQE ON (default)\n",
    "FOR LEARNING: Disable it to see exactly what Spark is doing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4: Understanding SQL Plans, Stages, and Shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HOW TO READ A SPARK SQL PLAN\n",
      "============================================================\n",
      "\n",
      "Go to the SQL Tab and click on the last query (3.3a or 3.3b).\n",
      "You'll see a Physical Plan like this:\n",
      "\n",
      "  == Physical Plan ==\n",
      "  * HashAggregate (5)          ← Step 5: Final count\n",
      "  +- Exchange (4)              ← Step 4: SHUFFLE!\n",
      "     +- * HashAggregate (3)    ← Step 3: Partial count\n",
      "        +- * Project (2)       ← Step 2: Calculate category\n",
      "           +- * Range (1)      ← Step 1: Generate numbers\n",
      "\n",
      "READ IT BOTTOM TO TOP! Data flows upward:\n",
      "  Step 1 → 2 → 3 → 4 → 5 → Result\n",
      "\n",
      "\n",
      "PLAN OPERATORS GLOSSARY\n",
      "=======================\n",
      "What do these operator names mean?\n",
      "\n",
      "  | Operator           | Spark Code              | Meaning                    |\n",
      "  |--------------------|-------------------------|----------------------------|\n",
      "  | Range              | spark.range()           | Generate numbers           |\n",
      "  | Scan / TableScan   | spark.read / table      | Read from file/table       |\n",
      "  | Project            | select() / withColumn() | Select or compute columns  |\n",
      "  | Filter             | filter() / where()      | Filter rows                |\n",
      "  | HashAggregate      | groupBy().agg()         | Group and aggregate        |\n",
      "  | Exchange           | (automatic)             | SHUFFLE! Stage boundary    |\n",
      "  | Sort               | orderBy()               | Sort data                  |\n",
      "  | BroadcastHashJoin  | join() (small table)    | Join with broadcast        |\n",
      "  | SortMergeJoin      | join() (large tables)   | Join with shuffle          |\n",
      "  | Union              | union()                 | Combine DataFrames         |\n",
      "\n",
      "\"Project\" = \"What columns do I want?\"\n",
      "  - select(\"a\", \"b\")           → Project [a, b]\n",
      "  - withColumn(\"new\", expr)    → Project [*, new]\n",
      "  - drop(\"col\")                → Project [all except col]\n",
      "\n",
      "\n",
      "HOW STAGES ARE CREATED\n",
      "======================\n",
      "Look for [codegen id : X] in the detailed plan:\n",
      "\n",
      "  (1) Range [codegen id : 1]        ─┐\n",
      "  (2) Project [codegen id : 1]       │ Same codegen = STAGE 4\n",
      "  (3) HashAggregate [codegen id : 1]─┘\n",
      "                                     \n",
      "  (4) Exchange                       ← SHUFFLE = STAGE BOUNDARY!\n",
      "                                     \n",
      "  (5) HashAggregate [codegen id : 2] ← New codegen = STAGE 5\n",
      "\n",
      "RULE: Everything between shuffles is ONE stage.\n",
      "      Exchange (shuffle) creates a new stage.\n",
      "\n",
      "\n",
      "WHAT IS A SHUFFLE?\n",
      "==================\n",
      "A shuffle is when Spark MOVES DATA between executors.\n",
      "\n",
      "Example: groupBy(\"category\")\n",
      "  - Data for category=1 might be on Executor A, B, and C\n",
      "  - To count category=1, all that data must go to ONE place\n",
      "  - This \"moving data around\" is the SHUFFLE\n",
      "\n",
      "         BEFORE SHUFFLE              AFTER SHUFFLE\n",
      "    ┌─────────────────────┐     ┌─────────────────────┐\n",
      "    │ Executor A          │     │ Executor A          │\n",
      "    │ cat=1, cat=2, cat=3 │     │ cat=1, cat=1, cat=1 │\n",
      "    ├─────────────────────┤ --> ├─────────────────────┤\n",
      "    │ Executor B          │     │ Executor B          │\n",
      "    │ cat=1, cat=2, cat=3 │     │ cat=2, cat=2, cat=2 │\n",
      "    ├─────────────────────┤     ├─────────────────────┤\n",
      "    │ Executor C          │     │ Executor C          │\n",
      "    │ cat=1, cat=2, cat=3 │     │ cat=3, cat=3, cat=3 │\n",
      "    └─────────────────────┘     └─────────────────────┘\n",
      "\n",
      "\n",
      "ARE SHUFFLES GOOD OR BAD?\n",
      "=========================\n",
      "Shuffles are NECESSARY but EXPENSIVE.\n",
      "\n",
      "NECESSARY because:\n",
      "  - groupBy needs all rows with same key together\n",
      "  - join needs matching rows from both tables together\n",
      "  - orderBy needs to compare all data\n",
      "  \n",
      "  Without shuffles, distributed computing wouldn't work!\n",
      "\n",
      "EXPENSIVE because:\n",
      "  - Data moves across the NETWORK (slow!)\n",
      "  - Data is written to DISK (I/O)\n",
      "  - Creates a stage boundary (synchronization)\n",
      "\n",
      "BEST PRACTICE:\n",
      "  ✓ Accept shuffles when needed (groupBy, join, orderBy)\n",
      "  ✗ Avoid UNNECESSARY shuffles\n",
      "  ✗ Don't shuffle more data than needed (filter early!)\n",
      "\n",
      "\n",
      "OPERATIONS THAT CAUSE SHUFFLES\n",
      "==============================\n",
      "  - groupBy()     → Groups need same-key data together\n",
      "  - join()        → Matching rows must meet\n",
      "  - orderBy()     → Global sorting needs all data\n",
      "  - repartition() → Explicitly redistributes data\n",
      "  - distinct()    → Needs to compare all values\n",
      "\n",
      "\n",
      "TIPS TO MINIMIZE SHUFFLE COST\n",
      "=============================\n",
      "1. FILTER EARLY - Remove rows before the shuffle\n",
      "2. SELECT ONLY NEEDED COLUMNS - Less data to move\n",
      "3. USE BROADCAST JOINS - For small tables (no shuffle!)\n",
      "4. CACHE IF REUSING - Avoid recomputing before shuffle\n",
      "\n",
      "We'll explore shuffles more in Part 6!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.4: Understanding SQL Plans, Stages, and Shuffles\n",
    "#\n",
    "# Let's learn how to read Spark's execution plan!\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"HOW TO READ A SPARK SQL PLAN\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Go to the SQL Tab and click on the last query (3.3a or 3.3b).\n",
    "You'll see a Physical Plan like this:\n",
    "\n",
    "  == Physical Plan ==\n",
    "  * HashAggregate (5)          ← Step 5: Final count\n",
    "  +- Exchange (4)              ← Step 4: SHUFFLE!\n",
    "     +- * HashAggregate (3)    ← Step 3: Partial count\n",
    "        +- * Project (2)       ← Step 2: Calculate category\n",
    "           +- * Range (1)      ← Step 1: Generate numbers\n",
    "\n",
    "READ IT BOTTOM TO TOP! Data flows upward:\n",
    "  Step 1 → 2 → 3 → 4 → 5 → Result\n",
    "\n",
    "\n",
    "PLAN OPERATORS GLOSSARY\n",
    "=======================\n",
    "What do these operator names mean?\n",
    "\n",
    "  | Operator           | Spark Code              | Meaning                    |\n",
    "  |--------------------|-------------------------|----------------------------|\n",
    "  | Range              | spark.range()           | Generate numbers           |\n",
    "  | Scan / TableScan   | spark.read / table      | Read from file/table       |\n",
    "  | Project            | select() / withColumn() | Select or compute columns  |\n",
    "  | Filter             | filter() / where()      | Filter rows                |\n",
    "  | HashAggregate      | groupBy().agg()         | Group and aggregate        |\n",
    "  | Exchange           | (automatic)             | SHUFFLE! Stage boundary    |\n",
    "  | Sort               | orderBy()               | Sort data                  |\n",
    "  | BroadcastHashJoin  | join() (small table)    | Join with broadcast        |\n",
    "  | SortMergeJoin      | join() (large tables)   | Join with shuffle          |\n",
    "  | Union              | union()                 | Combine DataFrames         |\n",
    "\n",
    "\"Project\" = \"What columns do I want?\"\n",
    "  - select(\"a\", \"b\")           → Project [a, b]\n",
    "  - withColumn(\"new\", expr)    → Project [*, new]\n",
    "  - drop(\"col\")                → Project [all except col]\n",
    "\n",
    "\n",
    "HOW STAGES ARE CREATED\n",
    "======================\n",
    "Look for [codegen id : X] in the detailed plan:\n",
    "\n",
    "  (1) Range [codegen id : 1]        ─┐\n",
    "  (2) Project [codegen id : 1]       │ Same codegen = STAGE 4\n",
    "  (3) HashAggregate [codegen id : 1]─┘\n",
    "                                     \n",
    "  (4) Exchange                       ← SHUFFLE = STAGE BOUNDARY!\n",
    "                                     \n",
    "  (5) HashAggregate [codegen id : 2] ← New codegen = STAGE 5\n",
    "\n",
    "RULE: Everything between shuffles is ONE stage.\n",
    "      Exchange (shuffle) creates a new stage.\n",
    "\n",
    "\n",
    "WHAT IS A SHUFFLE?\n",
    "==================\n",
    "A shuffle is when Spark MOVES DATA between executors.\n",
    "\n",
    "Example: groupBy(\"category\")\n",
    "  - Data for category=1 might be on Executor A, B, and C\n",
    "  - To count category=1, all that data must go to ONE place\n",
    "  - This \"moving data around\" is the SHUFFLE\n",
    "\n",
    "         BEFORE SHUFFLE              AFTER SHUFFLE\n",
    "    ┌─────────────────────┐     ┌─────────────────────┐\n",
    "    │ Executor A          │     │ Executor A          │\n",
    "    │ cat=1, cat=2, cat=2 │     │ cat=1, cat=1, cat=1 │\n",
    "    ├─────────────────────┤ --> ├─────────────────────┤\n",
    "    │ Executor B          │     │ Executor B          │\n",
    "    │ cat=3, cat=1, cat=3 │     │ cat=2, cat=2, cat=2 │\n",
    "    ├─────────────────────┤     ├─────────────────────┤\n",
    "    │ Executor C          │     │ Executor C          │\n",
    "    │ cat=1, cat=2, cat=3 │     │ cat=3, cat=3, cat=3 │\n",
    "    └─────────────────────┘     └─────────────────────┘\n",
    "\n",
    "\n",
    "ARE SHUFFLES GOOD OR BAD?\n",
    "=========================\n",
    "Shuffles are NECESSARY but EXPENSIVE.\n",
    "\n",
    "NECESSARY because:\n",
    "  - groupBy needs all rows with same key together\n",
    "  - join needs matching rows from both tables together\n",
    "  - orderBy needs to compare all data\n",
    "  \n",
    "  Without shuffles, distributed computing wouldn't work!\n",
    "\n",
    "EXPENSIVE because:\n",
    "  - Data moves across the NETWORK (slow!)\n",
    "  - Data is written to DISK (I/O)\n",
    "  - Creates a stage boundary (synchronization)\n",
    "\n",
    "BEST PRACTICE:\n",
    "  ✓ Accept shuffles when needed (groupBy, join, orderBy)\n",
    "  ✗ Avoid UNNECESSARY shuffles\n",
    "  ✗ Don't shuffle more data than needed (filter early!)\n",
    "\n",
    "\n",
    "OPERATIONS THAT CAUSE SHUFFLES\n",
    "==============================\n",
    "  - groupBy()     → Groups need same-key data together\n",
    "  - join()        → Matching rows must meet\n",
    "  - orderBy()     → Global sorting needs all data\n",
    "  - repartition() → Explicitly redistributes data\n",
    "  - distinct()    → Needs to compare all values\n",
    "\n",
    "\n",
    "TIPS TO MINIMIZE SHUFFLE COST\n",
    "=============================\n",
    "1. FILTER EARLY - Remove rows before the shuffle\n",
    "2. SELECT ONLY NEEDED COLUMNS - Less data to move\n",
    "3. USE BROADCAST JOINS - For small tables (no shuffle!)\n",
    "4. CACHE IF REUSING - Avoid recomputing before shuffle\n",
    "\n",
    "We'll explore shuffles more in Part 6!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Transformations are Lazy\n",
    "\n",
    "Now let's add **transformations** (filter, add columns, etc.)\n",
    "\n",
    "Remember: transformations are LAZY - they don't create jobs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1: Add transformations (NO jobs will be created!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformations added: filter + withColumn\n",
      "\n",
      "==================================================\n",
      "CHECK THE SPARK UI - JOBS TAB\n",
      "==================================================\n",
      "\n",
      "Did any NEW jobs appear?\n",
      "\n",
      "NO! The job count is the same as before.\n",
      "\n",
      "Spark is waiting. It knows we want to:\n",
      "1. Create numbers 0-99\n",
      "2. Filter to keep > 50\n",
      "3. Add a 'doubled' column\n",
      "\n",
      "But it won't DO any of this until we call an action.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1: Add transformations (NO jobs will be created!)\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Start with numbers 0-99\n",
    "df = spark.range(100)\n",
    "\n",
    "# Add transformations:\n",
    "# 1. Filter: keep only numbers > 50\n",
    "# 2. Add a new column: the number multiplied by 2\n",
    "df_transformed = df \\\n",
    "    .filter(col(\"id\") > 50) \\\n",
    "    .withColumn(\"doubled\", col(\"id\") * 2)\n",
    "\n",
    "print(\"Transformations added: filter + withColumn\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHECK THE SPARK UI - JOBS TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Did any NEW jobs appear?\n",
    "\n",
    "NO! The job count is the same as before.\n",
    "\n",
    "Spark is waiting. It knows we want to:\n",
    "1. Create numbers 0-99\n",
    "2. Filter to keep > 50\n",
    "3. Add a 'doubled' column\n",
    "\n",
    "But it won't DO any of this until we call an action.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: NOW trigger the action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted 49 rows\n",
      "\n",
      "First 5 rows:\n",
      "+---+-------+\n",
      "| id|doubled|\n",
      "+---+-------+\n",
      "| 51|    102|\n",
      "| 52|    104|\n",
      "| 53|    106|\n",
      "| 54|    108|\n",
      "| 55|    110|\n",
      "+---+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "==================================================\n",
      "CHECK THE SPARK UI - JOBS TAB\n",
      "==================================================\n",
      "\n",
      "Look for job: \"4.2: Count transformed data\"\n",
      "\n",
      "This ONE job did ALL the work:\n",
      "  1. Generated numbers 0-99\n",
      "  2. Filtered to keep only > 50  \n",
      "  3. Added the 'doubled' column\n",
      "  4. Counted the rows\n",
      "\n",
      "All the transformations were combined into ONE job!\n",
      "\n",
      "You'll also see \"4.2: Show the data\" - that's a SEPARATE action.\n",
      "Each action triggers its own job(s). We ran two actions:\n",
      "  - count() → 1 job\n",
      "  - show()  → 1-2 jobs (internal optimization)\n",
      "\n",
      "KEY INSIGHT:\n",
      "Spark waits until an action, then runs ALL transformations together.\n",
      "This allows Spark to optimize the entire pipeline.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.2: NOW trigger the action\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"4.2: Count transformed data\")\n",
    "\n",
    "# Using count() instead of show() to guarantee exactly 1 job\n",
    "# (show() can create multiple jobs due to internal optimizations)\n",
    "total = df_transformed.count()\n",
    "print(f\"Counted {total} rows\")\n",
    "\n",
    "# Let's also see the data (this is a SEPARATE action!)\n",
    "spark.sparkContext.setJobDescription(\"4.2: Show the data\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df_transformed.show(5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHECK THE SPARK UI - JOBS TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Look for job: \"4.2: Count transformed data\"\n",
    "\n",
    "This ONE job did ALL the work:\n",
    "  1. Generated numbers 0-99\n",
    "  2. Filtered to keep only > 50  \n",
    "  3. Added the 'doubled' column\n",
    "  4. Counted the rows\n",
    "\n",
    "All the transformations were combined into ONE job!\n",
    "\n",
    "You'll also see \"4.2: Show the data\" - that's a SEPARATE action.\n",
    "Each action triggers its own job(s). We ran two actions:\n",
    "  - count() → 1 job\n",
    "  - show()  → 1-2 jobs (internal optimization)\n",
    "\n",
    "KEY INSIGHT:\n",
    "Spark waits until an action, then runs ALL transformations together.\n",
    "This allows Spark to optimize the entire pipeline.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Looking Inside Jobs (Stages & Tasks)\n",
    "\n",
    "A **Job** is made of **Stages**, and each Stage has **Tasks**.\n",
    "\n",
    "```\n",
    "Job\n",
    " └── Stage 1\n",
    "      └── Task 1, Task 2, Task 3...\n",
    " └── Stage 2  \n",
    "      └── Task 1, Task 2, Task 3...\n",
    "```\n",
    "\n",
    "**Tasks** are the smallest unit of work. They run IN PARALLEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1: Compare show() vs count() vs take()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Count: 10000\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Take: [Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n",
      "\n",
      "==================================================\n",
      "COMPARE THESE THREE JOBS IN SPARK UI\n",
      "==================================================\n",
      "\n",
      "Find all three jobs and compare them:\n",
      "\n",
      "\"5.1a: show(5) - needs only 5 rows, but reads 6\":\n",
      "  - Click on the job, then click on the Stage\n",
      "  - Look at Input Records: shows 6 records!\n",
      "  - WHY? Spark fetches n+1 to check if there's more data\n",
      "\n",
      "\"5.1b: count() - needs ALL rows\":\n",
      "  - Look at Tasks: MULTIPLE tasks!\n",
      "  - Look at Input Records: 10,000 (all of them!)\n",
      "  - WHY? .count() must scan ALL the data\n",
      "\n",
      "\"5.1c: take(5) - exactly 5 rows\":\n",
      "  - Look at Input Records: exactly 5!\n",
      "  - WHY? .take(n) fetches exactly n rows\n",
      "\n",
      "KEY INSIGHT:\n",
      "Different actions have different costs!\n",
      "\n",
      "\n",
      "WHY DOES show(5) PROCESS 6 RECORDS?\n",
      "===================================\n",
      "Spark fetches n+1 rows to check if there's MORE data.\n",
      "This helps it decide whether to show:\n",
      "  \"only showing top 5 rows\"\n",
      "\n",
      "  | Action    | Fetches  | Displays | Why?                    |\n",
      "  |-----------|----------|----------|-------------------------|\n",
      "  | show(5)   | 6 rows   | 5 rows   | Checks if more exist    |\n",
      "  | take(5)   | 5 rows   | 5 rows   | Exactly what you asked  |\n",
      "  | count()   | ALL rows | 1 number | Must scan everything    |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.1: Compare show() vs count() vs take()\n",
    "\n",
    "df = spark.range(10_000)\n",
    "\n",
    "# First: show() - only needs a few rows\n",
    "spark.sparkContext.setJobDescription(\"5.1a: show(5) - reads 6 rows (?)\")\n",
    "df.show(5)\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Second: count() - needs ALL rows\n",
    "spark.sparkContext.setJobDescription(\"5.1b: count() - needs ALL rows\")\n",
    "total = df.count()\n",
    "print(f\"Count: {total}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Third: take() - exactly n rows\n",
    "spark.sparkContext.setJobDescription(\"5.1c: take(5) - exactly 5 rows\")\n",
    "rows = df.take(5)\n",
    "print(f\"Take: {rows}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARE THESE THREE JOBS IN SPARK UI\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Find all three jobs and compare them:\n",
    "\n",
    "\"5.1a: show(5) - needs only 5 rows, but reads 6\":\n",
    "  - Click on the job, then click on the Stage\n",
    "  - Look at Input Records: shows 6 records!\n",
    "  - WHY? Spark fetches n+1 to check if there's more data\n",
    "\n",
    "\"5.1b: count() - needs ALL rows\":\n",
    "  - Look at Tasks: MULTIPLE tasks!\n",
    "  - Look at Input Records: 10,000 (all of them!)\n",
    "  - WHY? .count() must scan ALL the data\n",
    "\n",
    "\"5.1c: take(5) - exactly 5 rows\":\n",
    "  - Look at Input Records: exactly 5!\n",
    "  - WHY? .take(n) fetches exactly n rows\n",
    "\n",
    "KEY INSIGHT:\n",
    "Different actions have different costs!\n",
    "\n",
    "\n",
    "WHY DOES show(5) PROCESS 6 RECORDS?\n",
    "===================================\n",
    "Spark fetches n+1 rows to check if there's MORE data.\n",
    "This helps it decide whether to show:\n",
    "  \"only showing top 5 rows\"\n",
    "\n",
    "  | Action    | Fetches  | Displays | Why?                    |\n",
    "  |-----------|----------|----------|-------------------------|\n",
    "  | show(5)   | 6 rows   | 5 rows   | Checks if more exist    |\n",
    "  | take(5)   | 5 rows   | 5 rows   | Exactly what you asked  |\n",
    "  | count()   | ALL rows | 1 number | Must scan everything    |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2: Explore a Stage's details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 50000\n",
      "\n",
      "==================================================\n",
      "EXPLORE THE STAGE DETAILS\n",
      "==================================================\n",
      "\n",
      "1. Go to Jobs tab, find \"5.2: Count 50,000 numbers\"\n",
      "\n",
      "2. Click on the job to see its details\n",
      "\n",
      "3. Click on the Stage number to see stage details\n",
      "\n",
      "4. Look for:\n",
      "   - Summary Metrics: min/median/max task duration\n",
      "   - Event Timeline: visual of when tasks ran\n",
      "   - Tasks table: one row per task\n",
      "\n",
      "5. In the Event Timeline, look at the colors:\n",
      "   - GREEN = Executor Computing Time (actual work)\n",
      "   - Other colors = overhead (scheduling, serialization, etc.)\n",
      "   - Ideally, most of the bar should be GREEN!\n",
      "\n",
      "\n",
      "UNDERSTANDING TASK COUNT\n",
      "========================\n",
      "The number of tasks depends on your PARALLELISM:\n",
      "  - More cores = more tasks running in parallel\n",
      "  - Each task processes a portion of the data\n",
      "\n",
      "Example with 50,000 rows:\n",
      "  | Cores | Tasks | Records per Task |\n",
      "  |-------|-------|------------------|\n",
      "  | 2     | 2     | 25,000           |\n",
      "  | 4     | 4     | 12,500           |\n",
      "  | 8     | 8     | 6,250            |\n",
      "\n",
      "Check \"Input Records\" in the Summary Metrics:\n",
      "  Total records = Tasks × Records per Task\n",
      "  \n",
      "In the Executor table at the bottom, you can see:\n",
      "  - Which executor ran the tasks\n",
      "  - Total Input Records processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.2: Explore a Stage's details\n",
    "\n",
    "# df = spark.range(50000)\n",
    "df = spark.range(50000, numPartitions=16)\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"5.2: Count 50,000 numbers\")\n",
    "total = df.count()\n",
    "print(f\"Total: {total}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORE THE STAGE DETAILS\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. Go to Jobs tab, find \"5.2: Count 50,000 numbers\"\n",
    "\n",
    "2. Click on the job to see its details\n",
    "\n",
    "3. Click on the Stage number to see stage details\n",
    "\n",
    "4. Look for:\n",
    "   - Summary Metrics: min/median/max task duration\n",
    "   - Event Timeline: visual of when tasks ran\n",
    "   - Tasks table: one row per task\n",
    "\n",
    "5. In the Event Timeline, look at the colors:\n",
    "   - GREEN = Executor Computing Time (actual work)\n",
    "   - Other colors = overhead (scheduling, serialization, etc.)\n",
    "   - Ideally, most of the bar should be GREEN!\n",
    "\n",
    "\n",
    "UNDERSTANDING TASK COUNT\n",
    "========================\n",
    "The number of tasks depends on your PARALLELISM:\n",
    "  - More cores = more tasks running in parallel\n",
    "  - Each task processes a portion of the data\n",
    "\n",
    "Example with 50,000 rows:\n",
    "  | Cores | Tasks | Records per Task |\n",
    "  |-------|-------|------------------|\n",
    "  | 2     | 2     | 25,000           |\n",
    "  | 4     | 4     | 12,500           |\n",
    "  | 8     | 8     | 6,250            |\n",
    "\n",
    "Check \"Input Records\" in the Summary Metrics:\n",
    "  Total records = Tasks × Records per Task\n",
    "  \n",
    "In the Executor table at the bottom, you can see:\n",
    "  - Which executor ran the tasks\n",
    "  - Total Input Records processed\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3: Number of cores vs input records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1: 4 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing Spark session\n",
    "if \"spark\" in globals():\n",
    "    globals()[\"spark\"].stop()\n",
    "\n",
    "# Create a new Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark-UI-Workshop\")\n",
    "    .master(\"spark://127.0.0.1:7077\")\n",
    "    .config(\"spark.ui.port\", \"4040\")\n",
    "    .config(\"spark.ui.host\", \"0.0.0.0\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"4\")\n",
    "    .config(\"spark.cores.max\", \"4\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = spark.range(50000)\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"5.3.1: Count 50,000 numbers - 4 cores\")\n",
    "total = df.count()\n",
    "print(f\"Total: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1: 2 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing Spark session\n",
    "if \"spark\" in globals():\n",
    "    globals()[\"spark\"].stop()\n",
    "\n",
    "# Create a new Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark-UI-Workshop\")\n",
    "    .master(\"spark://127.0.0.1:7077\")\n",
    "    .config(\"spark.ui.port\", \"4040\")\n",
    "    .config(\"spark.ui.host\", \"0.0.0.0\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.cores.max\", \"2\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = spark.range(50000)\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"5.3.2: Count 50,000 numbers - 2 cores\")\n",
    "total = df.count()\n",
    "print(f\"Total: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1: 16 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing Spark session\n",
    "if \"spark\" in globals():\n",
    "    globals()[\"spark\"].stop()\n",
    "\n",
    "# Create a new Spark session\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Spark-UI-Workshop\")\n",
    "    .master(\"spark://127.0.0.1:7077\")\n",
    "    .config(\"spark.ui.port\", \"4040\")\n",
    "    .config(\"spark.ui.host\", \"0.0.0.0\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"16\")\n",
    "    .config(\"spark.cores.max\", \"16\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "df = spark.range(50000)\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"5.3.1: Count 50,000 numbers - 16 cores\")\n",
    "total = df.count()\n",
    "print(f\"Total: {total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Shuffles Create New Stages\n",
    "\n",
    "So far, our jobs had only **one stage**.\n",
    "\n",
    "But some operations need to **move data around**. This is called a **shuffle**.\n",
    "\n",
    "Shuffles create **new stages**.\n",
    "\n",
    "Operations that cause shuffles:\n",
    "- `groupBy()` - grouping by a key\n",
    "- `orderBy()` - sorting all data  \n",
    "- `join()` - combining two tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1: Setup data for groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data:\n",
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       0|\n",
      "|  1|       1|\n",
      "|  2|       2|\n",
      "|  3|       3|\n",
      "|  4|       4|\n",
      "|  5|       0|\n",
      "|  6|       1|\n",
      "|  7|       2|\n",
      "|  8|       3|\n",
      "|  9|       4|\n",
      "| 10|       0|\n",
      "| 11|       1|\n",
      "| 12|       2|\n",
      "| 13|       3|\n",
      "| 14|       4|\n",
      "| 15|       0|\n",
      "| 16|       1|\n",
      "| 17|       2|\n",
      "| 18|       3|\n",
      "| 19|       4|\n",
      "+---+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "We have 1000 numbers, each assigned to category 0-4.\n",
      "Now let's COUNT how many numbers are in each category...\n",
      "This requires groupBy() - which causes a SHUFFLE!\n"
     ]
    }
   ],
   "source": [
    "# 6.1: Setup data for groupBy\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create numbers 0-999, each with a category (0, 1, 2, 3, or 4)\n",
    "df = spark.range(1000) \\\n",
    "    .withColumn(\"category\", (col(\"id\") % 5))\n",
    "\n",
    "print(\"Sample data:\")\n",
    "spark.sparkContext.setJobDescription(\"6.1: Show sample data\")\n",
    "df.show(20)\n",
    "\n",
    "print(\"\\nWe have 1000 numbers, each assigned to category 0-4.\")\n",
    "print(\"Now let's COUNT how many numbers are in each category...\")\n",
    "print(\"This requires groupBy() - which causes a SHUFFLE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2: Run groupBy (causes a shuffle!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CHECK THE SPARK UI - STAGES TAB\n",
      "==================================================\n",
      "\n",
      "Go to the STAGES tab and find the two stages for \"6.2: GroupBy\":\n",
      "\n",
      "STAGE 1 (e.g., Stage 37): BEFORE the shuffle\n",
      "=============================================\n",
      "  - Tasks: 8 (one per core/partition)\n",
      "  - Input Records: 1000 (your original data)\n",
      "  - Shuffle Write: ~2.4 KiB / 40 records\n",
      "  \n",
      "  WHAT HAPPENED:\n",
      "  - Each task read ~125 rows (1000 ÷ 8 tasks)\n",
      "  - Did PARTIAL count per category\n",
      "  - Wrote results to shuffle files (5 categories × 8 tasks = 40 records)\n",
      "\n",
      "\n",
      "STAGE 2 (e.g., Stage 38): AFTER the shuffle\n",
      "=============================================\n",
      "  - Tasks: 200 (default spark.sql.shuffle.partitions!)\n",
      "  - Shuffle Read: ~2.4 KiB / 40 records\n",
      "  - Most tasks read 0 records (empty partitions)\n",
      "  \n",
      "  WHAT HAPPENED:\n",
      "  - 200 partitions were created (Spark default)\n",
      "  - But we only have 5 categories!\n",
      "  - So only ~5 partitions have data, 195 are empty\n",
      "  - Each partition does FINAL count for its categories\n",
      "\n",
      "\n",
      "WHY 200 TASKS IF MOST PARTITIONS ARE EMPTY?\n",
      "===========================================\n",
      "Good question! Without AQE, Spark uses a STATIC plan:\n",
      "\n",
      "  1. BEFORE execution: Spark decides \"I'll use 200 shuffle partitions\"\n",
      "  2. DURING shuffle write: Data is hashed to partitions (only 5 get data)\n",
      "  3. DURING shuffle read: Spark launches ALL 200 tasks anyway\n",
      "  4. Each empty task: Quickly checks \"no data\" and completes (~3ms)\n",
      "\n",
      "  SHUFFLE WRITE (Stage 1):\n",
      "    Hash function decides which partition: hash(category) % 200\n",
      "    \n",
      "    cat=0 → partition 47  (has data)\n",
      "    cat=1 → partition 122 (has data)\n",
      "    cat=2 → partition 88  (has data)\n",
      "    ...\n",
      "    partition 0, 1, 2... → EMPTY (no categories hashed here)\n",
      "\n",
      "  SHUFFLE READ (Stage 2):\n",
      "    Spark launches 200 tasks (one per partition)\n",
      "    195 tasks: \"No data for me!\" → finish in ~3ms\n",
      "    5 tasks: \"I have data!\" → do the actual work\n",
      "\n",
      "Check the Stage 2 metrics - you'll see:\n",
      "  Duration: Min ~3ms (empty), Max ~100ms (has data)\n",
      "\n",
      "This is wasteful! That's why AQE helps - it detects empty\n",
      "partitions AFTER shuffle write and skips them in Stage 2.\n",
      "\n",
      "\n",
      "HOW SHUFFLE WORKS (DIAGRAM)\n",
      "===========================\n",
      "\n",
      "Examp`le: groupBy(\"category\") with categories A, B, C`\n",
      "\n",
      "  STAGE 1: Each partition has mixed data\n",
      "  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐\n",
      "  │ Partition 1 │ │ Partition 2 │ │ Partition 3 │\n",
      "  │ A=10, B=5   │ │ A=8, C=12   │ │ B=7, C=3    │\n",
      "  │ C=2         │ │ B=4         │ │ A=6         │\n",
      "  └─────────────┘ └─────────────┘ └─────────────┘\n",
      "         │               │               │\n",
      "         └───────────────┼───────────────┘\n",
      "                         │\n",
      "                    S H U F F L E\n",
      "                  (data reorganized by key)\n",
      "                         │\n",
      "         ┌───────────────┼───────────────┐\n",
      "         │               │               │\n",
      "         ▼               ▼               ▼\n",
      "  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐\n",
      "  │ Partition A │ │ Partition B │ │ Partition C │\n",
      "  │ A=10        │ │ B=5         │ │ C=2         │\n",
      "  │ A=8         │ │ B=4         │ │ C=12        │\n",
      "  │ A=6         │ │ B=7         │ │ C=3         │\n",
      "  │ ─────────── │ │ ─────────── │ │ ─────────── │\n",
      "  │ Total: 24   │ │ Total: 16   │ │ Total: 17   │\n",
      "  └─────────────┘ └─────────────┘ └─────────────┘\n",
      "  STAGE 2: Each partition has ONE category (can compute final total)\n",
      "\n",
      "\n",
      "PARTITIONS vs EXECUTORS vs TASKS\n",
      "================================\n",
      "These terms are related but different:\n",
      "\n",
      "  PARTITIONS = Logical chunks of data (how data is divided)\n",
      "  TASKS      = Work units (1 task processes 1 partition)  \n",
      "  CORES      = How many tasks run in parallel per executor\n",
      "  EXECUTORS  = Physical workers (machines/processes)\n",
      "\n",
      "  ┌──────────────────────────────────────────────────────┐\n",
      "  │                      EXECUTOR                        │\n",
      "  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │\n",
      "  │  │ Core 1  │  │ Core 2  │  │ Core 3  │  │ Core 4  │  │\n",
      "  │  │  Task   │  │  Task   │  │  Task   │  │  Task   │  │\n",
      "  │  │   ↓     │  │   ↓     │  │   ↓     │  │   ↓     │  │\n",
      "  │  │ Part 1  │  │ Part 2  │  │ Part 3  │  │ Part 4  │  │\n",
      "  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │\n",
      "  └──────────────────────────────────────────────────────┘\n",
      "\n",
      "  1 Task = 1 Partition (always)\n",
      "  1 Core = 1 Task at a time\n",
      "  1 Executor = Multiple cores = Multiple parallel tasks\n",
      "\n",
      "\n",
      "SPARK ARCHITECTURE: DRIVER vs EXECUTORS\n",
      "=======================================\n",
      "\n",
      "  ┌─────────────────────────────────────────────────────────────┐\n",
      "  │                         DRIVER                              │\n",
      "  │                   (your notebook/app)                       │\n",
      "  │                                                             │\n",
      "  │  • Creates execution plan (DAG)                             │\n",
      "  │  • Schedules tasks on executors                             │\n",
      "  │  • Tracks progress & handles failures                       │\n",
      "  │  • Does NOT move data during shuffle!                       │\n",
      "  │  • Receives final results (e.g., collect())                 │\n",
      "  └─────────────────────────┬───────────────────────────────────┘\n",
      "                            │ coordinates\n",
      "              ┌─────────────┼─────────────┐\n",
      "              │             │             │\n",
      "              ▼             ▼             ▼\n",
      "  ┌───────────────┐ ┌───────────────┐ ┌───────────────┐\n",
      "  │  EXECUTOR 1   │ │  EXECUTOR 2   │ │  EXECUTOR 3   │\n",
      "  │               │ │               │ │               │\n",
      "  │ • Runs tasks  │ │ • Runs tasks  │ │ • Runs tasks  │\n",
      "  │ • Stores data │ │ • Stores data │ │ • Stores data │\n",
      "  │ • Shuffle I/O │ │ • Shuffle I/O │ │ • Shuffle I/O │\n",
      "  └───────┬───────┘ └───────┬───────┘ └───────┬───────┘\n",
      "          │                 │                 │\n",
      "          └────────────────►│◄────────────────┘\n",
      "                    SHUFFLE DATA\n",
      "               (executors talk directly!)\n",
      "\n",
      "  KEY INSIGHT: \n",
      "  During shuffle, data moves BETWEEN EXECUTORS directly.\n",
      "  The driver only coordinates - it doesn't touch the data!\n",
      "\n",
      "\n",
      "SHUFFLE: SINGLE vs MULTIPLE EXECUTORS\n",
      "=====================================\n",
      "\n",
      "In this workshop, we have 1 EXECUTOR with multiple cores:\n",
      "\n",
      "  ┌────────────────┐\n",
      "  │     DRIVER     │ ◄── Your notebook\n",
      "  └───────┬────────┘\n",
      "          │ coordinates\n",
      "          ▼\n",
      "  ┌───────────────────────────────────────────┐\n",
      "  │              SINGLE EXECUTOR              │\n",
      "  │  ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐  │\n",
      "  │  │Core 1 │ │Core 2 │ │Core 3 │ │Core 4 │  │\n",
      "  │  │Task 1 │ │Task 2 │ │Task 3 │ │Task 4 │  │\n",
      "  │  └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘  │\n",
      "  │      │         │         │         │      │\n",
      "  │      └────┬────┴────┬────┴────┬────┘      │\n",
      "  │           ▼         ▼         ▼           │\n",
      "  │      ┌─────────────────────────────┐      │\n",
      "  │      │     LOCAL DISK (shuffle)    │      │\n",
      "  │      └─────────────────────────────┘      │\n",
      "  └───────────────────────────────────────────┘\n",
      "  \n",
      "  Shuffle = DISK I/O only (fast)\n",
      "\n",
      "\n",
      "In PRODUCTION with multiple executors:\n",
      "\n",
      "  ┌────────────────┐\n",
      "  │     DRIVER     │\n",
      "  └───────┬────────┘\n",
      "          │ coordinates\n",
      "     ┌────┴────┐\n",
      "     ▼         ▼\n",
      "  ┌─────────────┐         ┌─────────────┐\n",
      "  │ EXECUTOR 1  │         │ EXECUTOR 2  │\n",
      "  │ ┌───┐ ┌───┐ │         │ ┌───┐ ┌───┐ │\n",
      "  │ │T1 │ │T2 │ │         │ │T3 │ │T4 │ │\n",
      "  │ └─┬─┘ └─┬─┘ │         │ └─┬─┘ └─┬─┘ │\n",
      "  │   └──┬──┘   │         │   └──┬──┘   │\n",
      "  │      ▼      │         │      ▼      │\n",
      "  │  [Disk 1]   │◄───────►│  [Disk 2]   │\n",
      "  └─────────────┘ NETWORK └─────────────┘\n",
      "  \n",
      "  Shuffle = DISK I/O + NETWORK I/O (slow!)\n",
      "\n",
      "This is why \"shuffles are expensive\" - in real clusters,\n",
      "data must travel over the network between machines!\n",
      "\n",
      "\n",
      "HOW TO REDUCE SHUFFLE PARTITIONS\n",
      "================================\n",
      "Spark uses 200 shuffle partitions by default:\n",
      "  spark.sql.shuffle.partitions = 200\n",
      "\n",
      "With only 5 categories, most partitions are empty!\n",
      "Solutions:\n",
      "  1. Enable AQE (coalesces automatically) - disabled for learning\n",
      "  2. Manually set: spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
      "\n",
      "HOW TO CHOOSE PARTITION COUNT?\n",
      "  - Match your data: 5 categories → 5 partitions\n",
      "  - Or use your cores: 8 cores → 8 partitions (for parallelism)\n",
      "  - Rule of thumb: 2-4 partitions per core for large datasets\n",
      "  \n",
      "  Too few  → Less parallelism, slower\n",
      "  Too many → Empty partitions, overhead\n",
      "\n",
      "\n",
      "SHUFFLE WRITE vs SHUFFLE READ\n",
      "=============================\n",
      "  Stage 1: Shuffle Write = Data SENT (2.4 KiB)\n",
      "           ↓ (data moves across network/disk)\n",
      "  Stage 2: Shuffle Read  = Data RECEIVED (2.4 KiB)\n",
      "  \n",
      "  These should match! Data written = Data read.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.2: Run groupBy (causes a shuffle!)\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"6.2: GroupBy - causes SHUFFLE (5 categories / 5 partitions)\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "df = spark.range(1000) \\\n",
    "    .withColumn(\"category\", (col(\"id\") % 5))\n",
    "result = df.groupBy(\"category\").agg(count(\"*\").alias(\"total\"))\n",
    "result.collect()\n",
    "\n",
    "# spark.sparkContext.setJobDescription(\"6.2 (show): GroupBy results\")\n",
    "# result.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHECK THE SPARK UI - STAGES TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Go to the STAGES tab and find the two stages for \"6.2: GroupBy\":\n",
    "\n",
    "STAGE 1 (e.g., Stage 37): BEFORE the shuffle\n",
    "=============================================\n",
    "  - Tasks: 8 (one per core/partition)\n",
    "  - Input Records: 1000 (your original data)\n",
    "  - Shuffle Write: ~2.4 KiB / 40 records\n",
    "  \n",
    "  WHAT HAPPENED:\n",
    "  - Each task read ~125 rows (1000 ÷ 8 tasks)\n",
    "  - Did PARTIAL count per category\n",
    "  - Wrote results to shuffle files (5 categories × 8 tasks = 40 records)\n",
    "\n",
    "\n",
    "STAGE 2 (e.g., Stage 38): AFTER the shuffle\n",
    "=============================================\n",
    "  - Tasks: 200 (default spark.sql.shuffle.partitions!)\n",
    "  - Shuffle Read: ~2.4 KiB / 40 records\n",
    "  - Most tasks read 0 records (empty partitions)\n",
    "  \n",
    "  WHAT HAPPENED:\n",
    "  - 200 partitions were created (Spark default)\n",
    "  - But we only have 5 categories!\n",
    "  - So only ~5 partitions have data, 195 are empty\n",
    "  - Each partition does FINAL count for its categories\n",
    "\n",
    "\n",
    "WHY 200 TASKS IF MOST PARTITIONS ARE EMPTY?\n",
    "===========================================\n",
    "Good question! Without AQE, Spark uses a STATIC plan:\n",
    "\n",
    "  1. BEFORE execution: Spark decides \"I'll use 200 shuffle partitions\"\n",
    "  2. DURING shuffle write: Data is hashed to partitions (only 5 get data)\n",
    "  3. DURING shuffle read: Spark launches ALL 200 tasks anyway\n",
    "  4. Each empty task: Quickly checks \"no data\" and completes (~3ms)\n",
    "\n",
    "  SHUFFLE WRITE (Stage 1):\n",
    "    Hash function decides which partition: hash(category) % 200\n",
    "    \n",
    "    cat=0 → partition 47  (has data)\n",
    "    cat=1 → partition 122 (has data)\n",
    "    cat=2 → partition 88  (has data)\n",
    "    ...\n",
    "    partition 0, 1, 2... → EMPTY (no categories hashed here)\n",
    "\n",
    "  SHUFFLE READ (Stage 2):\n",
    "    Spark launches 200 tasks (one per partition)\n",
    "    195 tasks: \"No data for me!\" → finish in ~3ms\n",
    "    5 tasks: \"I have data!\" → do the actual work\n",
    "\n",
    "Check the Stage 2 metrics - you'll see:\n",
    "  Duration: Min ~3ms (empty), Max ~100ms (has data)\n",
    "\n",
    "This is wasteful! That's why AQE helps - it detects empty\n",
    "partitions AFTER shuffle write and skips them in Stage 2.\n",
    "\n",
    "\n",
    "HOW SHUFFLE WORKS (DIAGRAM)\n",
    "===========================\n",
    "\n",
    "Examp`le: groupBy(\"category\") with categories A, B, C`\n",
    "\n",
    "  STAGE 1: Each partition has mixed data\n",
    "  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐\n",
    "  │ Partition 1 │ │ Partition 2 │ │ Partition 3 │\n",
    "  │ A=10, B=5   │ │ A=8, C=12   │ │ B=7, C=3    │\n",
    "  │ C=2         │ │ B=4         │ │ A=6         │\n",
    "  └─────────────┘ └─────────────┘ └─────────────┘\n",
    "         │               │               │\n",
    "         └───────────────┼───────────────┘\n",
    "                         │\n",
    "                    S H U F F L E\n",
    "                  (data reorganized by key)\n",
    "                         │\n",
    "         ┌───────────────┼───────────────┐\n",
    "         │               │               │\n",
    "         ▼               ▼               ▼\n",
    "  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐\n",
    "  │ Partition A │ │ Partition B │ │ Partition C │\n",
    "  │ A=10        │ │ B=5         │ │ C=2         │\n",
    "  │ A=8         │ │ B=4         │ │ C=12        │\n",
    "  │ A=6         │ │ B=7         │ │ C=3         │\n",
    "  │ ─────────── │ │ ─────────── │ │ ─────────── │\n",
    "  │ Total: 24   │ │ Total: 16   │ │ Total: 17   │\n",
    "  └─────────────┘ └─────────────┘ └─────────────┘\n",
    "  STAGE 2: Each partition has ONE category (can compute final total)\n",
    "\n",
    "\n",
    "PARTITIONS vs EXECUTORS vs TASKS\n",
    "================================\n",
    "These terms are related but different:\n",
    "\n",
    "  PARTITIONS = Logical chunks of data (how data is divided)\n",
    "  TASKS      = Work units (1 task processes 1 partition)  \n",
    "  CORES      = How many tasks run in parallel per executor\n",
    "  EXECUTORS  = Physical workers (machines/processes)\n",
    "\n",
    "  ┌──────────────────────────────────────────────────────┐\n",
    "  │                      EXECUTOR                        │\n",
    "  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  │\n",
    "  │  │ Core 1  │  │ Core 2  │  │ Core 3  │  │ Core 4  │  │\n",
    "  │  │  Task   │  │  Task   │  │  Task   │  │  Task   │  │\n",
    "  │  │   ↓     │  │   ↓     │  │   ↓     │  │   ↓     │  │\n",
    "  │  │ Part 1  │  │ Part 2  │  │ Part 3  │  │ Part 4  │  │\n",
    "  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  │\n",
    "  └──────────────────────────────────────────────────────┘\n",
    "\n",
    "  1 Task = 1 Partition (always)\n",
    "  1 Core = 1 Task at a time\n",
    "  1 Executor = Multiple cores = Multiple parallel tasks\n",
    "\n",
    "\n",
    "SPARK ARCHITECTURE: DRIVER vs EXECUTORS\n",
    "=======================================\n",
    "\n",
    "  ┌─────────────────────────────────────────────────────────────┐\n",
    "  │                         DRIVER                              │\n",
    "  │                   (your notebook/app)                       │\n",
    "  │                                                             │\n",
    "  │  • Creates execution plan (DAG)                             │\n",
    "  │  • Schedules tasks on executors                             │\n",
    "  │  • Tracks progress & handles failures                       │\n",
    "  │  • Does NOT move data during shuffle!                       │\n",
    "  │  • Receives final results (e.g., collect())                 │\n",
    "  └─────────────────────────┬───────────────────────────────────┘\n",
    "                            │ coordinates\n",
    "              ┌─────────────┼─────────────┐\n",
    "              │             │             │\n",
    "              ▼             ▼             ▼\n",
    "  ┌───────────────┐ ┌───────────────┐ ┌───────────────┐\n",
    "  │  EXECUTOR 1   │ │  EXECUTOR 2   │ │  EXECUTOR 3   │\n",
    "  │               │ │               │ │               │\n",
    "  │ • Runs tasks  │ │ • Runs tasks  │ │ • Runs tasks  │\n",
    "  │ • Stores data │ │ • Stores data │ │ • Stores data │\n",
    "  │ • Shuffle I/O │ │ • Shuffle I/O │ │ • Shuffle I/O │\n",
    "  └───────┬───────┘ └───────┬───────┘ └───────┬───────┘\n",
    "          │                 │                 │\n",
    "          └────────────────►│◄────────────────┘\n",
    "                    SHUFFLE DATA\n",
    "               (executors talk directly!)\n",
    "\n",
    "  KEY INSIGHT: \n",
    "  During shuffle, data moves BETWEEN EXECUTORS directly.\n",
    "  The driver only coordinates - it doesn't touch the data!\n",
    "\n",
    "\n",
    "SHUFFLE: SINGLE vs MULTIPLE EXECUTORS\n",
    "=====================================\n",
    "\n",
    "In this workshop, we have 1 EXECUTOR with multiple cores:\n",
    "\n",
    "  ┌────────────────┐\n",
    "  │     DRIVER     │ ◄── Your notebook\n",
    "  └───────┬────────┘\n",
    "          │ coordinates\n",
    "          ▼\n",
    "  ┌───────────────────────────────────────────┐\n",
    "  │              SINGLE EXECUTOR              │\n",
    "  │  ┌───────┐ ┌───────┐ ┌───────┐ ┌───────┐  │\n",
    "  │  │Core 1 │ │Core 2 │ │Core 3 │ │Core 4 │  │\n",
    "  │  │Task 1 │ │Task 2 │ │Task 3 │ │Task 4 │  │\n",
    "  │  └───┬───┘ └───┬───┘ └───┬───┘ └───┬───┘  │\n",
    "  │      │         │         │         │      │\n",
    "  │      └────┬────┴────┬────┴────┬────┘      │\n",
    "  │           ▼         ▼         ▼           │\n",
    "  │      ┌─────────────────────────────┐      │\n",
    "  │      │     LOCAL DISK (shuffle)    │      │\n",
    "  │      └─────────────────────────────┘      │\n",
    "  └───────────────────────────────────────────┘\n",
    "  \n",
    "  Shuffle = DISK I/O only (fast)\n",
    "\n",
    "\n",
    "In PRODUCTION with multiple executors:\n",
    "\n",
    "  ┌────────────────┐\n",
    "  │     DRIVER     │\n",
    "  └───────┬────────┘\n",
    "          │ coordinates\n",
    "     ┌────┴────┐\n",
    "     ▼         ▼\n",
    "  ┌─────────────┐         ┌─────────────┐\n",
    "  │ EXECUTOR 1  │         │ EXECUTOR 2  │\n",
    "  │ ┌───┐ ┌───┐ │         │ ┌───┐ ┌───┐ │\n",
    "  │ │T1 │ │T2 │ │         │ │T3 │ │T4 │ │\n",
    "  │ └─┬─┘ └─┬─┘ │         │ └─┬─┘ └─┬─┘ │\n",
    "  │   └──┬──┘   │         │   └──┬──┘   │\n",
    "  │      ▼      │         │      ▼      │\n",
    "  │  [Disk 1]   │◄───────►│  [Disk 2]   │\n",
    "  └─────────────┘ NETWORK └─────────────┘\n",
    "  \n",
    "  Shuffle = DISK I/O + NETWORK I/O (slow!)\n",
    "\n",
    "This is why \"shuffles are expensive\" - in real clusters,\n",
    "data must travel over the network between machines!\n",
    "\n",
    "\n",
    "HOW TO REDUCE SHUFFLE PARTITIONS\n",
    "================================\n",
    "Spark uses 200 shuffle partitions by default:\n",
    "  spark.sql.shuffle.partitions = 200\n",
    "\n",
    "With only 5 categories, most partitions are empty!\n",
    "Solutions:\n",
    "  1. Enable AQE (coalesces automatically) - disabled for learning\n",
    "  2. Manually set: spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "HOW TO CHOOSE PARTITION COUNT?\n",
    "  - Match your data: 5 categories → 5 partitions\n",
    "  - Or use your cores: 8 cores → 8 partitions (for parallelism)\n",
    "  - Rule of thumb: 2-4 partitions per core for large datasets\n",
    "  \n",
    "  Too few  → Less parallelism, slower\n",
    "  Too many → Empty partitions, overhead\n",
    "\n",
    "\n",
    "SHUFFLE WRITE vs SHUFFLE READ\n",
    "=============================\n",
    "  Stage 1: Shuffle Write = Data SENT (2.4 KiB)\n",
    "           ↓ (data moves across network/disk)\n",
    "  Stage 2: Shuffle Read  = Data RECEIVED (2.4 KiB)\n",
    "  \n",
    "  These should match! Data written = Data read.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: The SQL Tab\n",
    "\n",
    "The **SQL tab** shows you a visual diagram of HOW Spark executes your query.\n",
    "\n",
    "This is extremely useful for understanding what's happening!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1: Total sales by store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|store|    total_sales|\n",
      "+-----+---------------+\n",
      "|    0| 99999950000000|\n",
      "|    1| 99999970000000|\n",
      "|    3|100000010000000|\n",
      "|    2| 99999990000000|\n",
      "|    4|100000030000000|\n",
      "+-----+---------------+\n",
      "\n",
      "\n",
      "==================================================\n",
      "EXPLORE THE SQL TAB\n",
      "==================================================\n",
      "\n",
      "1. Go to the SQL tab in Spark UI\n",
      "2. Click on the most recent query\n",
      "3. You'll see a DAG (diagram) and detailed metrics\n",
      "\n",
      "\n",
      "THE PHYSICAL PLAN\n",
      "=================\n",
      "Read bottom to top - data flows upward:\n",
      "\n",
      "  == Physical Plan ==\n",
      "  * HashAggregate (5)      ← FINAL sum (5 output rows = 5 stores)\n",
      "  +- Exchange (4)          ← SHUFFLE (200 partitions)\n",
      "     +- * HashAggregate (3) ← PARTIAL sum (40 rows = 5 stores × 8 tasks)\n",
      "        +- * Project (2)    ← Calculate store and amount columns\n",
      "           +- * Range (1)   ← Generate 10,000 numbers\n",
      "\n",
      "\n",
      "KEY METRICS TO LOOK FOR\n",
      "=======================\n",
      "\n",
      "RANGE (Step 1):\n",
      "  \"number of output rows: 10,000\"\n",
      "  → Generated 10,000 numbers (0 to 9,999)\n",
      "\n",
      "PROJECT (Step 2):\n",
      "  → Calculated: store = id % 5, amount = id * 10\n",
      "  → No row count shown (same as input: 10,000)\n",
      "\n",
      "HASH AGGREGATE - PARTIAL (Step 3):\n",
      "  \"number of output rows: 40\"\n",
      "  → Why 40? = 5 stores × 8 tasks\n",
      "  → Each task computed partial sum for each store it saw\n",
      "\n",
      "EXCHANGE - THE SHUFFLE (Step 4):\n",
      "  \"shuffle records written: 40\"    ← Data sent (matches partial agg output)\n",
      "  \"records read: 40\"               ← Data received (should match!)\n",
      "  \"number of partitions: 200\"      ← Default shuffle partitions\n",
      "  \"shuffle bytes written: 2.4 KiB\" ← Total data shuffled\n",
      "  \"local bytes read: 2.4 KiB\"      ← Read from LOCAL disk (same executor)\n",
      "  \"remote bytes read: 0.0 B\"       ← No NETWORK transfer (single executor!)\n",
      "\n",
      "HASH AGGREGATE - FINAL (Step 5):\n",
      "  \"number of output rows: 5\"\n",
      "  → Final result: 5 stores with total sales each\n",
      "\n",
      "\n",
      "UNDERSTANDING MIN/MED/MAX METRICS\n",
      "=================================\n",
      "Many metrics show (min, med, max) across tasks:\n",
      "\n",
      "  \"shuffle write time: total 484ms (min 0ms, med 0ms, max 85ms)\"\n",
      "  \n",
      "  This means:\n",
      "  - Total across all tasks: 484ms\n",
      "  - Minimum task: 0ms (empty partition, no data)\n",
      "  - Median task: 0ms (most partitions empty!)\n",
      "  - Maximum task: 85ms (partition with data)\n",
      "\n",
      "  The big gap between median (0ms) and max (85ms) shows\n",
      "  that most of the 200 partitions were EMPTY!\n",
      "\n",
      "\n",
      "WHAT IS SPILL? (IMPORTANT!)\n",
      "===========================\n",
      "Spill happens when Spark RUNS OUT OF MEMORY during an operation.\n",
      "\n",
      "  NORMAL (data fits in memory):\n",
      "  ┌─────────────────────────────────┐\n",
      "  │           MEMORY                │\n",
      "  │  ┌─────────────────────────┐   │\n",
      "  │  │   Hash Table for        │   │\n",
      "  │  │   Aggregation           │   │\n",
      "  │  │   (all data fits!)      │   │\n",
      "  │  └─────────────────────────┘   │\n",
      "  └─────────────────────────────────┘\n",
      "           ↓ Fast!\n",
      "\n",
      "\n",
      "  SPILL (data doesn't fit in memory):\n",
      "  ┌─────────────────────────────────┐\n",
      "  │           MEMORY                │\n",
      "  │  ┌─────────────────────────┐   │\n",
      "  │  │   Hash Table (FULL!)    │   │\n",
      "  │  └───────────┬─────────────┘   │\n",
      "  └──────────────│──────────────────┘\n",
      "                 │ Overflow!\n",
      "                 ▼\n",
      "  ┌─────────────────────────────────┐\n",
      "  │            DISK                 │\n",
      "  │  ┌─────────────────────────┐   │\n",
      "  │  │   Spilled data          │   │  ← SLOW!\n",
      "  │  │   (temporary files)     │   │\n",
      "  │  └─────────────────────────┘   │\n",
      "  └─────────────────────────────────┘\n",
      "\n",
      "When spill happens:\n",
      "  - groupBy() → hash table too large\n",
      "  - join()    → join buffers overflow  \n",
      "  - orderBy() → sort buffers overflow\n",
      "\n",
      "Why it's BAD:\n",
      "  - Disk is 100-1000x SLOWER than memory\n",
      "  - Data written to disk, then read back\n",
      "  - Significantly slows down your job\n",
      "\n",
      "In this query:\n",
      "  \"spill size: 0.0 B\" → GOOD! Everything fit in memory\n",
      "\n",
      "If you see spill size > 0, try:\n",
      "  1. Increase executor memory: spark.executor.memory\n",
      "  2. Reduce data per partition (more partitions)\n",
      "  3. Filter data earlier to reduce size\n",
      "\n",
      "\n",
      "MEMORY METRICS\n",
      "==============\n",
      "  \"peak memory: 55.0 MiB (min 256 KiB, med 256 KiB, max 1280 KiB)\"\n",
      "  → Memory used for hash tables during aggregation\n",
      "  → Shows how much memory each task needed\n",
      "\n",
      "\n",
      "LOCAL vs REMOTE READS\n",
      "=====================\n",
      "  \"local bytes read: 2.4 KiB\"   ← Data from SAME executor\n",
      "  \"remote bytes read: 0.0 B\"    ← Data from OTHER executors\n",
      "\n",
      "  In our single-executor setup, all reads are LOCAL.\n",
      "  In production with multiple executors:\n",
      "  - remote bytes > 0 means network transfer\n",
      "  - High remote bytes = expensive shuffle\n",
      "\n",
      "\n",
      "WHOLESTAGECODEGEN\n",
      "=================\n",
      "  \"WholeStageCodegen (1) duration: 240ms\"\n",
      "  \"WholeStageCodegen (2) duration: 899ms\"\n",
      "\n",
      "  Spark compiles multiple operators into optimized Java code.\n",
      "  [codegen id : 1] = Range + Project + Partial HashAggregate\n",
      "  [codegen id : 2] = Final HashAggregate\n",
      "  \n",
      "  The * before operators means they're part of codegen:\n",
      "    * HashAggregate  ← In codegen (fast!)\n",
      "    Exchange         ← NOT in codegen (shuffle can't be compiled)\n",
      "\n",
      "\n",
      "WHAT TO LOOK FOR (QUICK REFERENCE)\n",
      "==================================\n",
      "| Metric                  | Good          | Warning Sign        |\n",
      "|-------------------------|---------------|---------------------|\n",
      "| spill size              | 0 B           | > 0 (memory issue)  |\n",
      "| remote bytes read       | Low           | High (network cost) |\n",
      "| number of partitions    | Matches data  | 200 with few keys   |\n",
      "| output rows             | Expected      | Unexpected count    |\n",
      "| peak memory             | Reasonable    | Very high           |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7.1: Reading the SQL tab\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Create sales data: store (0-4), amount\n",
    "df = spark.range(10000000) \\\n",
    "    .withColumn(\"store\", (col(\"id\") % 5)) \\\n",
    "    .withColumn(\"amount\", col(\"id\") * 10)\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"7.1: Total sales by store\")\n",
    "\n",
    "result = df.groupBy(\"store\").agg(\n",
    "    sum(\"amount\").alias(\"total_sales\")\n",
    ")\n",
    "result.collect()\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"7.1 (show): Total sales by store\")\n",
    "result.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORE THE SQL TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. Go to the SQL tab in Spark UI\n",
    "2. Click on the most recent query\n",
    "3. You'll see a DAG (diagram) and detailed metrics\n",
    "\n",
    "\n",
    "THE PHYSICAL PLAN\n",
    "=================\n",
    "Read bottom to top - data flows upward:\n",
    "\n",
    "  == Physical Plan ==\n",
    "  * HashAggregate (5)      ← FINAL sum (5 output rows = 5 stores)\n",
    "  +- Exchange (4)          ← SHUFFLE (200 partitions)\n",
    "     +- * HashAggregate (3) ← PARTIAL sum (40 rows = 5 stores × 8 tasks)\n",
    "        +- * Project (2)    ← Calculate store and amount columns\n",
    "           +- * Range (1)   ← Generate 10,000 numbers\n",
    "\n",
    "\n",
    "KEY METRICS TO LOOK FOR\n",
    "=======================\n",
    "\n",
    "RANGE (Step 1):\n",
    "  \"number of output rows: 10,000\"\n",
    "  → Generated 10,000 numbers (0 to 9,999)\n",
    "\n",
    "PROJECT (Step 2):\n",
    "  → Calculated: store = id % 5, amount = id * 10\n",
    "  → No row count shown (same as input: 10,000)\n",
    "\n",
    "HASH AGGREGATE - PARTIAL (Step 3):\n",
    "  \"number of output rows: 40\"\n",
    "  → Why 40? = 5 stores × 8 tasks\n",
    "  → Each task computed partial sum for each store it saw\n",
    "\n",
    "EXCHANGE - THE SHUFFLE (Step 4):\n",
    "  \"shuffle records written: 40\"    ← Data sent (matches partial agg output)\n",
    "  \"records read: 40\"               ← Data received (should match!)\n",
    "  \"number of partitions: 200\"      ← Default shuffle partitions\n",
    "  \"shuffle bytes written: 2.4 KiB\" ← Total data shuffled\n",
    "  \"local bytes read: 2.4 KiB\"      ← Read from LOCAL disk (same executor)\n",
    "  \"remote bytes read: 0.0 B\"       ← No NETWORK transfer (single executor!)\n",
    "\n",
    "HASH AGGREGATE - FINAL (Step 5):\n",
    "  \"number of output rows: 5\"\n",
    "  → Final result: 5 stores with total sales each\n",
    "\n",
    "\n",
    "UNDERSTANDING MIN/MED/MAX METRICS\n",
    "=================================\n",
    "Many metrics show (min, med, max) across tasks:\n",
    "\n",
    "  \"shuffle write time: total 484ms (min 0ms, med 0ms, max 85ms)\"\n",
    "  \n",
    "  This means:\n",
    "  - Total across all tasks: 484ms\n",
    "  - Minimum task: 0ms (empty partition, no data)\n",
    "  - Median task: 0ms (most partitions empty!)\n",
    "  - Maximum task: 85ms (partition with data)\n",
    "\n",
    "  The big gap between median (0ms) and max (85ms) shows\n",
    "  that most of the 200 partitions were EMPTY!\n",
    "\n",
    "\n",
    "WHAT IS SPILL? (IMPORTANT!)\n",
    "===========================\n",
    "Spill happens when Spark RUNS OUT OF MEMORY during an operation.\n",
    "\n",
    "  NORMAL (data fits in memory):\n",
    "  ┌───────────────────────────────┐\n",
    "  │           MEMORY              │\n",
    "  │  ┌─────────────────────────┐  │\n",
    "  │  │   Hash Table for        │  │\n",
    "  │  │   Aggregation           │  │\n",
    "  │  │   (all data fits!)      │  │\n",
    "  │  └─────────────────────────┘  │\n",
    "  └───────────────────────────────┘\n",
    "           ↓ Fast!\n",
    "\n",
    "\n",
    "  SPILL (data doesn't fit in memory):\n",
    "  ┌───────────────────────────────┐\n",
    "  │           MEMORY              │\n",
    "  │  ┌─────────────────────────┐  │\n",
    "  │  │   Hash Table (FULL!)    │  │\n",
    "  │  └───────────┬─────────────┘  │\n",
    "  └──────────────│────────────────┘\n",
    "                 │ Overflow!\n",
    "                 ▼\n",
    "  ┌───────────────────────────────┐\n",
    "  │            DISK               │\n",
    "  │  ┌─────────────────────────┐  │\n",
    "  │  │   Spilled data          │  │  ← SLOW!\n",
    "  │  │   (temporary files)     │  │\n",
    "  │  └─────────────────────────┘  │\n",
    "  └───────────────────────────────┘\n",
    "\n",
    "When spill happens:\n",
    "  - groupBy() → hash table too large\n",
    "  - join()    → join buffers overflow  \n",
    "  - orderBy() → sort buffers overflow\n",
    "\n",
    "Why it's BAD:\n",
    "  - Disk is 100-1000x SLOWER than memory\n",
    "  - Data written to disk, then read back\n",
    "  - Significantly slows down your job\n",
    "\n",
    "In this query:\n",
    "  \"spill size: 0.0 B\" → GOOD! Everything fit in memory\n",
    "\n",
    "If you see spill size > 0, try:\n",
    "  1. Increase executor memory: spark.executor.memory\n",
    "  2. Reduce data per partition (more partitions)\n",
    "  3. Filter data earlier to reduce size\n",
    "\n",
    "\n",
    "MEMORY METRICS\n",
    "==============\n",
    "  \"peak memory: 55.0 MiB (min 256 KiB, med 256 KiB, max 1280 KiB)\"\n",
    "  → Memory used for hash tables during aggregation\n",
    "  → Shows how much memory each task needed\n",
    "\n",
    "\n",
    "LOCAL vs REMOTE READS\n",
    "=====================\n",
    "  \"local bytes read: 2.4 KiB\"   ← Data from SAME executor\n",
    "  \"remote bytes read: 0.0 B\"    ← Data from OTHER executors\n",
    "\n",
    "  In our single-executor setup, all reads are LOCAL.\n",
    "  In production with multiple executors:\n",
    "  - remote bytes > 0 means network transfer\n",
    "  - High remote bytes = expensive shuffle\n",
    "\n",
    "\n",
    "WHOLESTAGECODEGEN\n",
    "=================\n",
    "  \"WholeStageCodegen (1) duration: 240ms\"\n",
    "  \"WholeStageCodegen (2) duration: 899ms\"\n",
    "\n",
    "  Spark compiles multiple operators into optimized Java code.\n",
    "  [codegen id : 1] = Range + Project + Partial HashAggregate\n",
    "  [codegen id : 2] = Final HashAggregate\n",
    "  \n",
    "  The * before operators means they're part of codegen:\n",
    "    * HashAggregate  ← In codegen (fast!)\n",
    "    Exchange         ← NOT in codegen (shuffle can't be compiled)\n",
    "\n",
    "\n",
    "WHAT TO LOOK FOR (QUICK REFERENCE)\n",
    "==================================\n",
    "| Metric                  | Good          | Warning Sign        |\n",
    "|-------------------------|---------------|---------------------|\n",
    "| spill size              | 0 B           | > 0 (memory issue)  |\n",
    "| remote bytes read       | Low           | High (network cost) |\n",
    "| number of partitions    | Matches data  | 200 with few keys   |\n",
    "| output rows             | Expected      | Unexpected count    |\n",
    "| peak memory             | Reasonable    | Very high           |\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2: Making Spark SPILL to disk (demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEMONSTRATING MEMORY SPILL\n",
      "============================================================\n",
      "\n",
      "Creating 5 million rows with 1 million unique keys...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Check the SQL tab for spill metrics.\n",
      "\n",
      "============================================================\n",
      "CHECK THE SQL TAB FOR SPILL\n",
      "============================================================\n",
      "\n",
      "1. Go to SQL tab, find \"7.2: High cardinality groupBy (causes SPILL)\"\n",
      "2. Look at the HashAggregate metrics - there are TWO:\n",
      "\n",
      "   PARTIAL HashAggregate (Step 3 - BEFORE shuffle):\n",
      "   ================================================\n",
      "   \"spill size: ~456 MiB\"              ← IT SPILLED!\n",
      "   \"peak memory: ~96 MiB\"              ← Memory limit per task\n",
      "   \"number of output rows: 5,000,000\"  ← All rows processed\n",
      "   \"number of sort fallback tasks: 8\"  ← Tasks that used disk!\n",
      "\n",
      "   FINAL HashAggregate (Step 5 - AFTER shuffle):\n",
      "   ==============================================\n",
      "   \"spill size: 0.0 B\"                 ← No spill here\n",
      "   \"peak memory: ~250 MiB\"             \n",
      "   \"number of output rows: 1,000,000\"  ← 1M unique groups\n",
      "\n",
      "\n",
      "WHY DID THE PARTIAL AGGREGATE SPILL?\n",
      "====================================\n",
      "The PARTIAL aggregation (before shuffle) processes ALL 5M rows:\n",
      "  - Each of 8 tasks processes ~625,000 rows\n",
      "  - Each task sees ~125,000 unique keys (1M keys / 8 tasks)\n",
      "  - Hash table for 125,000 keys doesn't fit in task memory\n",
      "  - Spark SPILLS overflow data to disk!\n",
      "\n",
      "The FINAL aggregation (after shuffle) didn't spill because:\n",
      "  - Data is already partially aggregated\n",
      "  - Each of 200 partitions has fewer entries to handle\n",
      "\n",
      "\n",
      "WHAT IS \"SORT FALLBACK TASKS\"?\n",
      "==============================\n",
      "\"number of sort fallback tasks: 8\"\n",
      "\n",
      "When hash aggregation runs out of memory, Spark falls back\n",
      "to SORT-BASED aggregation:\n",
      "  1. Spill data to disk\n",
      "  2. Sort the spilled data by key\n",
      "  3. Aggregate sorted data (sequential scan)\n",
      "\n",
      "This is SLOWER than hash aggregation but uses less memory.\n",
      "All 8 tasks had to use this fallback = significant spill!\n",
      "\n",
      "\n",
      "WHY DID IT SPILL NOW BUT NOT IN 7.1?\n",
      "====================================\n",
      "  ┌──────────────────────────────────────────────────────┐\n",
      "  │  7.1: Low Cardinality      │  7.2: High Cardinality  │\n",
      "  │  (5 unique keys)           │  (1M unique keys)       │\n",
      "  │                            │                         │\n",
      "  │  Hash Table per task:      │  Hash Table per task:   │\n",
      "  │  ┌─────┐                   │  ┌─────────────────┐    │\n",
      "  │  │ 0   │                   │  │ 0, 1, 2, 3...   │    │\n",
      "  │  │ 1   │  ← 5 entries!     │  │ ...             │    │\n",
      "  │  │ 2   │                   │  │ ... 124,997     │    │\n",
      "  │  │ 3   │                   │  │ ... 124,998     │    │\n",
      "  │  │ 4   │                   │  │ ... 124,999     │    │\n",
      "  │  └─────┘                   │  └─────────────────┘    │\n",
      "  │                            │  ← 125,000 entries!     │\n",
      "  │  ~1 KB memory              │  ~12 MB memory per task │\n",
      "  │                            │         │               │\n",
      "  │  Fits in memory ✓          │         ▼ SPILL!        │\n",
      "  │                            │      [DISK: 456 MiB]    │\n",
      "  └──────────────────────────────────────────────────────┘\n",
      "\n",
      "\n",
      "KEY INSIGHT: CARDINALITY MATTERS!\n",
      "=================================\n",
      "Low cardinality groupBy (few unique keys):\n",
      "  → Small hash table → No spill → Fast!\n",
      "\n",
      "High cardinality groupBy (many unique keys):  \n",
      "  → Large hash table → May spill → Slower!\n",
      "\n",
      "Look at the timing difference:\n",
      "  - 7.1: ~1 second (no spill)\n",
      "  - 7.2: ~10 seconds (456 MiB spilled!)\n",
      "\n",
      "This is why knowing your data cardinality is important!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7.2: Making Spark SPILL to disk (demonstration)\n",
    "#\n",
    "# In 7.1, we had 10,000 rows but only 5 stores.\n",
    "# The hash table only needed 5 entries - easy to fit in memory!\n",
    "#\n",
    "# To cause SPILL, we need HIGH CARDINALITY (many unique keys).\n",
    "\n",
    "from pyspark.sql.functions import col, count, sum as spark_sum\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DEMONSTRATING MEMORY SPILL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create data with HIGH CARDINALITY - many unique keys!\n",
    "# 5 million rows with 1 million unique keys\n",
    "print(\"\\nCreating 5 million rows with 1 million unique keys...\")\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"7.2: High cardinality groupBy (causes SPILL)\")\n",
    "\n",
    "df_spill = spark.range(5000000) \\\n",
    "    .withColumn(\"group_key\", (col(\"id\") % 1000000)) \\\n",
    "    .withColumn(\"value\", col(\"id\") * 10)\n",
    "\n",
    "# This groupBy needs a hash table with 1 MILLION entries!\n",
    "result = df_spill.groupBy(\"group_key\").agg(\n",
    "    count(\"*\").alias(\"cnt\"),\n",
    "    spark_sum(\"value\").alias(\"total\")\n",
    ")\n",
    "\n",
    "result.collect()\n",
    "print(\"Done! Check the SQL tab for spill metrics.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECK THE SQL TAB FOR SPILL\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "1. Go to SQL tab, find \"7.2: High cardinality groupBy (causes SPILL)\"\n",
    "2. Look at the HashAggregate metrics - there are TWO:\n",
    "\n",
    "   PARTIAL HashAggregate (Step 3 - BEFORE shuffle):\n",
    "   ================================================\n",
    "   \"spill size: ~456 MiB\"              ← IT SPILLED!\n",
    "   \"peak memory: ~96 MiB\"              ← Memory limit per task\n",
    "   \"number of output rows: 5,000,000\"  ← All rows processed\n",
    "   \"number of sort fallback tasks: 8\"  ← Tasks that used disk!\n",
    "\n",
    "   FINAL HashAggregate (Step 5 - AFTER shuffle):\n",
    "   ==============================================\n",
    "   \"spill size: 0.0 B\"                 ← No spill here\n",
    "   \"peak memory: ~250 MiB\"             \n",
    "   \"number of output rows: 1,000,000\"  ← 1M unique groups\n",
    "\n",
    "\n",
    "WHY DID THE PARTIAL AGGREGATE SPILL?\n",
    "====================================\n",
    "The PARTIAL aggregation (before shuffle) processes ALL 5M rows:\n",
    "  - Each of 8 tasks processes ~625,000 rows\n",
    "  - Each task sees ~125,000 unique keys (1M keys / 8 tasks)\n",
    "  - Hash table for 125,000 keys doesn't fit in task memory\n",
    "  - Spark SPILLS overflow data to disk!\n",
    "\n",
    "The FINAL aggregation (after shuffle) didn't spill because:\n",
    "  - Data is already partially aggregated\n",
    "  - Each of 200 partitions has fewer entries to handle\n",
    "\n",
    "\n",
    "WHAT IS \"SORT FALLBACK TASKS\"?\n",
    "==============================\n",
    "\"number of sort fallback tasks: 8\"\n",
    "\n",
    "When hash aggregation runs out of memory, Spark falls back\n",
    "to SORT-BASED aggregation:\n",
    "  1. Spill data to disk\n",
    "  2. Sort the spilled data by key\n",
    "  3. Aggregate sorted data (sequential scan)\n",
    "\n",
    "This is SLOWER than hash aggregation but uses less memory.\n",
    "All 8 tasks had to use this fallback = significant spill!\n",
    "\n",
    "\n",
    "WHY DID IT SPILL NOW BUT NOT IN 7.1?\n",
    "====================================\n",
    "  ┌──────────────────────────────────────────────────────┐\n",
    "  │  7.1: Low Cardinality      │  7.2: High Cardinality  │\n",
    "  │  (5 unique keys)           │  (1M unique keys)       │\n",
    "  │                            │                         │\n",
    "  │  Hash Table per task:      │  Hash Table per task:   │\n",
    "  │  ┌─────┐                   │  ┌─────────────────┐    │\n",
    "  │  │ 0   │                   │  │ 0, 1, 2, 3...   │    │\n",
    "  │  │ 1   │  ← 5 entries!     │  │ ...             │    │\n",
    "  │  │ 2   │                   │  │ ... 124,997     │    │\n",
    "  │  │ 3   │                   │  │ ... 124,998     │    │\n",
    "  │  │ 4   │                   │  │ ... 124,999     │    │\n",
    "  │  └─────┘                   │  └─────────────────┘    │\n",
    "  │                            │  ← 125,000 entries!     │\n",
    "  │  ~1 KB memory              │  ~12 MB memory per task │\n",
    "  │                            │         │               │\n",
    "  │  Fits in memory ✓          │         ▼ SPILL!        │\n",
    "  │                            │      [DISK: 456 MiB]    │\n",
    "  └──────────────────────────────────────────────────────┘\n",
    "\n",
    "\n",
    "KEY INSIGHT: CARDINALITY MATTERS!\n",
    "=================================\n",
    "Low cardinality groupBy (few unique keys):\n",
    "  → Small hash table → No spill → Fast!\n",
    "\n",
    "High cardinality groupBy (many unique keys):  \n",
    "  → Large hash table → May spill → Slower!\n",
    "\n",
    "Look at the timing difference:\n",
    "  - 7.1: ~1 second (no spill)\n",
    "  - 7.2: ~10 seconds (456 MiB spilled!)\n",
    "\n",
    "This is why knowing your data cardinality is important!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3: How to FIX spill - and when AQE can hurt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WHEN AQE CAN MAKE THINGS WORSE\n",
      "============================================================\n",
      "\n",
      "Running same query with AQE ON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Check SQL tab - compare with 7.2\n",
      "\n",
      "============================================================\n",
      "WHY AQE MADE IT WORSE!\n",
      "============================================================\n",
      "\n",
      "Compare the FINAL HashAggregate spill:\n",
      "\n",
      "  7.2 (AQE OFF): 200 partitions → spill size: 0 B      ✓\n",
      "  7.3 (AQE ON):  8 partitions   → spill size: 52 MiB  ✗\n",
      "\n",
      "WHAT HAPPENED?\n",
      "==============\n",
      "AQE saw:   \"200 partitions with ~250KB each - too small!\"\n",
      "AQE did:   Coalesced 200 → 8 partitions\n",
      "Result:    Each partition now has 125K keys → SPILL!\n",
      "\n",
      "  WITHOUT AQE (200 partitions):\n",
      "  ┌─────┐ ┌─────┐ ┌─────┐     ┌─────┐\n",
      "  │5000 │ │5000 │ │5000 │ ... │5000 │  ← 5K keys each\n",
      "  │keys │ │keys │ │keys │     │keys │     (fits in memory!)\n",
      "  └─────┘ └─────┘ └─────┘     └─────┘\n",
      "     200 small partitions = OK\n",
      "\n",
      "  WITH AQE (coalesced to 8 partitions):\n",
      "  ┌───────────────────────────────────┐\n",
      "  │         125,000 keys              │  ← 125K keys each\n",
      "  │    (TOO BIG - SPILLS TO DISK!)    │     (doesn't fit!)\n",
      "  └───────────────────────────────────┘\n",
      "     8 large partitions = SPILL!\n",
      "\n",
      "AQE optimizes for SHUFFLE OVERHEAD (fewer small tasks)\n",
      "but doesn't consider AGGREGATION MEMORY needs!\n",
      "\n",
      "\n",
      "HOW TO FIX SPILL\n",
      "================\n",
      "\n",
      "OPTION 1: MORE PARTITIONS\n",
      "=========================\n",
      "More partitions = Less data per task = Smaller hash tables\n",
      "\n",
      "  ┌─────────────────────────────────────────────────────────┐\n",
      "  │  100 partitions          vs      500 partitions         │\n",
      "  │                                                         │\n",
      "  │  1M keys ÷ 100 = 10K keys/task   1M ÷ 500 = 2K keys/task│\n",
      "  │                                                         │\n",
      "  │  Hash table: ~10K entries        Hash table: ~2K entries│\n",
      "  │  Memory: ~1 MB per task          Memory: ~200 KB/task   │\n",
      "  │                                                         │\n",
      "  │  May spill!                      Fits easily! ✓         │\n",
      "  └─────────────────────────────────────────────────────────┘\n",
      "\n",
      "To increase partitions:\n",
      "  spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
      "\n",
      "Trade-off:\n",
      "  ✓ Less memory per task → No spill\n",
      "  ✗ More tasks → More scheduling overhead\n",
      "  \n",
      "Best for: High cardinality data\n",
      "\n",
      "\n",
      "OPTION 2: MORE MEMORY\n",
      "=====================\n",
      "More memory = Larger hash tables fit = No spill\n",
      "\n",
      "  ┌─────────────────────────────────────────────────────────┐\n",
      "  │  512 MB executor         vs      2 GB executor          │\n",
      "  │                                                         │\n",
      "  │  Task memory: ~50 MB             Task memory: ~200 MB   │\n",
      "  │  Hash table limit: ~30 MB        Hash table: ~120 MB    │\n",
      "  │                                                         │\n",
      "  │  125K keys = ~12 MB              125K keys = ~12 MB     │\n",
      "  │  + overhead = SPILL!             + overhead = FITS! ✓   │\n",
      "  └─────────────────────────────────────────────────────────┘\n",
      "\n",
      "To increase memory:\n",
      "  spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
      "\n",
      "Or increase the fraction used for execution:\n",
      "  spark.conf.set(\"spark.memory.fraction\", \"0.8\")  # default is 0.6\n",
      "\n",
      "Trade-off:\n",
      "  ✓ More memory per task → No spill\n",
      "  ✗ Need more RAM on cluster\n",
      "  \n",
      "Best for: You have available RAM\n",
      "\n",
      "\n",
      "OPTION 3: DISABLE AQE COALESCING\n",
      "================================\n",
      "Keep AQE benefits but prevent aggressive coalescing:\n",
      "\n",
      "  spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
      "\n",
      "This keeps:\n",
      "  ✓ AQE's dynamic join optimization\n",
      "  ✓ AQE's skew handling\n",
      "  \n",
      "But disables:\n",
      "  ✗ Partition coalescing (which caused our problem!)\n",
      "\n",
      "\n",
      "OPTION 4: FILTER AND SELECT EARLY\n",
      "=================================\n",
      "Reduce data BEFORE the groupBy:\n",
      "\n",
      "  # Bad: groupBy on full data\n",
      "  df.groupBy(\"key\").agg(sum(\"value\"))\n",
      "  \n",
      "  # Better: filter first\n",
      "  df.filter(col(\"date\") > \"2024-01-01\") \\\n",
      "    .select(\"key\", \"value\") \\\n",
      "    .groupBy(\"key\").agg(sum(\"value\"))\n",
      "\n",
      "Less data = Smaller hash tables = Less spill\n",
      "\n",
      "\n",
      "SUMMARY: WHEN TO USE WHAT\n",
      "=========================\n",
      "| Problem | Solution |\n",
      "|---------|----------|\n",
      "| High cardinality groupBy | More partitions |\n",
      "| Cluster has spare RAM | Increase executor memory |\n",
      "| AQE coalescing too much | Disable coalescing |\n",
      "| Processing full table | Filter/select early |\n",
      "| Very wide rows | Select only needed columns |\n",
      "\n",
      "\n",
      "KEY INSIGHT\n",
      "===========\n",
      "Spill happens when: Data per task > Task memory\n",
      "\n",
      "Fix by either:\n",
      "  1. Reduce data per task (more partitions, filter early)\n",
      "  2. Increase task memory (more executor memory)\n",
      "\n",
      "AQE is usually helpful, but for high cardinality aggregations,\n",
      "it can coalesce partitions too aggressively and cause spill!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7.3: How to FIX spill - and when AQE can hurt!\n",
    "#\n",
    "# In 7.2 we saw spill with AQE OFF.\n",
    "# What if we try AQE ON? Let's see...\n",
    "\n",
    "from pyspark.sql.functions import col, count, sum as spark_sum\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"WHEN AQE CAN MAKE THINGS WORSE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Enable AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "print(\"\\nRunning same query with AQE ON...\")\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"7.3a: High cardinality with AQE ON\")\n",
    "\n",
    "df_spill = spark.range(5000000) \\\n",
    "    .withColumn(\"group_key\", (col(\"id\") % 1000000)) \\\n",
    "    .withColumn(\"value\", col(\"id\") * 10)\n",
    "\n",
    "result = df_spill.groupBy(\"group_key\").agg(\n",
    "    count(\"*\").alias(\"cnt\"),\n",
    "    spark_sum(\"value\").alias(\"total\")\n",
    ")\n",
    "result.collect()\n",
    "\n",
    "print(\"Done! Check SQL tab - compare with 7.2\")\n",
    "\n",
    "# Reset AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WHY AQE MADE IT WORSE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Compare the FINAL HashAggregate spill:\n",
    "\n",
    "  7.2 (AQE OFF): 200 partitions → spill size: 0 B      ✓\n",
    "  7.3 (AQE ON):  8 partitions   → spill size: 52 MiB  ✗\n",
    "\n",
    "WHAT HAPPENED?\n",
    "==============\n",
    "AQE saw:   \"200 partitions with ~250KB each - too small!\"\n",
    "AQE did:   Coalesced 200 → 8 partitions\n",
    "Result:    Each partition now has 125K keys → SPILL!\n",
    "\n",
    "  WITHOUT AQE (200 partitions):\n",
    "  ┌─────┐ ┌─────┐ ┌─────┐     ┌─────┐\n",
    "  │5000 │ │5000 │ │5000 │ ... │5000 │  ← 5K keys each\n",
    "  │keys │ │keys │ │keys │     │keys │     (fits in memory!)\n",
    "  └─────┘ └─────┘ └─────┘     └─────┘\n",
    "     200 small partitions = OK\n",
    "\n",
    "  WITH AQE (coalesced to 8 partitions):\n",
    "  ┌───────────────────────────────────┐\n",
    "  │         125,000 keys              │  ← 125K keys each\n",
    "  │    (TOO BIG - SPILLS TO DISK!)    │     (doesn't fit!)\n",
    "  └───────────────────────────────────┘\n",
    "     8 large partitions = SPILL!\n",
    "\n",
    "AQE optimizes for SHUFFLE OVERHEAD (fewer small tasks)\n",
    "but doesn't consider AGGREGATION MEMORY needs!\n",
    "\n",
    "\n",
    "HOW TO FIX SPILL\n",
    "================\n",
    "\n",
    "OPTION 1: MORE PARTITIONS\n",
    "=========================\n",
    "More partitions = Less data per task = Smaller hash tables\n",
    "\n",
    "  ┌─────────────────────────────────────────────────────────┐\n",
    "  │  100 partitions          vs      500 partitions         │\n",
    "  │                                                         │\n",
    "  │  1M keys ÷ 100 = 10K keys/task   1M ÷ 500 = 2K keys/task│\n",
    "  │                                                         │\n",
    "  │  Hash table: ~10K entries        Hash table: ~2K entries│\n",
    "  │  Memory: ~1 MB per task          Memory: ~200 KB/task   │\n",
    "  │                                                         │\n",
    "  │  May spill!                      Fits easily! ✓         │\n",
    "  └─────────────────────────────────────────────────────────┘\n",
    "\n",
    "To increase partitions:\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", \"500\")\n",
    "\n",
    "Trade-off:\n",
    "  ✓ Less memory per task → No spill\n",
    "  ✗ More tasks → More scheduling overhead\n",
    "  \n",
    "Best for: High cardinality data\n",
    "\n",
    "\n",
    "OPTION 2: MORE MEMORY\n",
    "=====================\n",
    "More memory = Larger hash tables fit = No spill\n",
    "\n",
    "  ┌─────────────────────────────────────────────────────────┐\n",
    "  │  512 MB executor         vs      2 GB executor          │\n",
    "  │                                                         │\n",
    "  │  Task memory: ~50 MB             Task memory: ~200 MB   │\n",
    "  │  Hash table limit: ~30 MB        Hash table: ~120 MB    │\n",
    "  │                                                         │\n",
    "  │  125K keys = ~12 MB              125K keys = ~12 MB     │\n",
    "  │  + overhead = SPILL!             + overhead = FITS! ✓   │\n",
    "  └─────────────────────────────────────────────────────────┘\n",
    "\n",
    "To increase memory:\n",
    "  spark.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "\n",
    "Or increase the fraction used for execution:\n",
    "  spark.conf.set(\"spark.memory.fraction\", \"0.8\")  # default is 0.6\n",
    "\n",
    "Trade-off:\n",
    "  ✓ More memory per task → No spill\n",
    "  ✗ Need more RAM on cluster\n",
    "  \n",
    "Best for: You have available RAM\n",
    "\n",
    "\n",
    "OPTION 3: DISABLE AQE COALESCING\n",
    "================================\n",
    "Keep AQE benefits but prevent aggressive coalescing:\n",
    "\n",
    "  spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"false\")\n",
    "\n",
    "This keeps:\n",
    "  ✓ AQE's dynamic join optimization\n",
    "  ✓ AQE's skew handling\n",
    "  \n",
    "But disables:\n",
    "  ✗ Partition coalescing (which caused our problem!)\n",
    "\n",
    "\n",
    "OPTION 4: FILTER AND SELECT EARLY\n",
    "=================================\n",
    "Reduce data BEFORE the groupBy:\n",
    "\n",
    "  # Bad: groupBy on full data\n",
    "  df.groupBy(\"key\").agg(sum(\"value\"))\n",
    "  \n",
    "  # Better: filter first\n",
    "  df.filter(col(\"date\") > \"2024-01-01\") \\\\\n",
    "    .select(\"key\", \"value\") \\\\\n",
    "    .groupBy(\"key\").agg(sum(\"value\"))\n",
    "\n",
    "Less data = Smaller hash tables = Less spill\n",
    "\n",
    "\n",
    "SUMMARY: WHEN TO USE WHAT\n",
    "=========================\n",
    "+-------------------------------------------------------+    \n",
    "| Problem                  | Solution                   |\n",
    "|--------------------------|----------------------------|\n",
    "| High cardinality groupBy | More partitions            |\n",
    "| Cluster has spare RAM    | Increase executor memory   |\n",
    "| AQE coalescing too much  | Disable coalescing         |\n",
    "| Processing full table    | Filter/select early        |\n",
    "| Very wide rows           | Select only needed columns |\n",
    "+-------------------------------------------------------+\n",
    "\n",
    "KEY INSIGHT\n",
    "===========\n",
    "Spill happens when: Data per task > Task memory\n",
    "\n",
    "Fix by either:\n",
    "  1. Reduce data per task (more partitions, filter early)\n",
    "  2. Increase task memory (more executor memory)\n",
    "\n",
    "AQE is usually helpful, but for high cardinality aggregations,\n",
    "it can coalesce partitions too aggressively and cause spill!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4: Reading SQL Plans to Find Optimizations\n",
    "\n",
    "The SQL plan tells you EXACTLY what Spark will do. Learning to read it helps you spot performance issues.\n",
    "\n",
    "**Common pattern to look for:** Check if your parallelism matches your available cores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SCENARIO: We increased from 2 cores to 16 cores\n",
      "          but the query got SLOWER! Why?\n",
      "======================================================================\n",
      "\n",
      "--- BAD: Using default partitions ---\n",
      "\n",
      "Physical Plan (look for 'splits'):\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[group_key#247L], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(group_key#247L, 200), ENSURE_REQUIREMENTS, [plan_id=514]\n",
      "   +- *(1) HashAggregate(keys=[group_key#247L], functions=[partial_count(1)])\n",
      "      +- *(1) Project [(id#245L % 1000000) AS group_key#247L]\n",
      "         +- *(1) Range (0, 5000000, step=1, splits=8)\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "--- GOOD: Explicit partitions = 16 (matching our cores) ---\n",
      "\n",
      "Physical Plan (look for 'splits'):\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[group_key#265L], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(group_key#265L, 200), ENSURE_REQUIREMENTS, [plan_id=543]\n",
      "   +- *(1) HashAggregate(keys=[group_key#265L], functions=[partial_count(1)])\n",
      "      +- *(1) Project [(id#263L % 1000000) AS group_key#265L]\n",
      "         +- *(1) Range (0, 5000000, step=1, splits=16)\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "WHAT TO LOOK FOR IN THE PLAN\n",
      "======================================================================\n",
      "\n",
      "In the Range operator, find 'splits=Some(N)':\n",
      "\n",
      "  BAD:  Range (0, 5000000, step=1, splits=Some(2))\n",
      "        ^\n",
      "        Only 2 partitions! 14 cores sitting idle!\n",
      "\n",
      "  GOOD: Range (0, 5000000, step=1, splits=Some(16))\n",
      "        ^\n",
      "        16 partitions! All cores working!\n",
      "\n",
      "\n",
      "THE RULE:\n",
      "=========\n",
      "    Partitions >= Available Cores\n",
      "\n",
      "    Otherwise, cores sit idle waiting for work!\n",
      "\n",
      "    ┌─────────────────────────────────────────────────┐\n",
      "    │  16 Cores with 2 Partitions                     │\n",
      "    │  ┌────┐ ┌────┐ ┌────┐ ... ┌────┐               │\n",
      "    │  │BUSY│ │BUSY│ │IDLE│     │IDLE│  ← 14 idle!   │\n",
      "    │  └────┘ └────┘ └────┘     └────┘               │\n",
      "    └─────────────────────────────────────────────────┘\n",
      "\n",
      "    ┌─────────────────────────────────────────────────┐\n",
      "    │  16 Cores with 16 Partitions                    │\n",
      "    │  ┌────┐ ┌────┐ ┌────┐ ... ┌────┐               │\n",
      "    │  │BUSY│ │BUSY│ │BUSY│     │BUSY│  ← all work!  │\n",
      "    │  └────┘ └────┘ └────┘     └────┘               │\n",
      "    └─────────────────────────────────────────────────┘\n",
      "\n",
      "\n",
      "HOW TO FIX PARTITION COUNT:\n",
      "===========================\n",
      "\n",
      "1. For spark.range():\n",
      "   spark.range(0, 5000000, 1, numPartitions=16)\n",
      "\n",
      "2. For reading files:\n",
      "   spark.read.option(\"maxPartitionBytes\", \"128MB\").parquet(path)\n",
      "\n",
      "3. For any DataFrame (adds a shuffle!):\n",
      "   df.repartition(16)\n",
      "\n",
      "4. Set default parallelism at session start:\n",
      "   .config(\"spark.default.parallelism\", \"16\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7.4: Reading SQL Plans to Find Optimizations\n",
    "#\n",
    "# One common mistake: Adding more cores but not more partitions!\n",
    "# The SQL plan reveals this issue clearly.\n",
    "\n",
    "from pyspark.sql.functions import col, count, sum\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SCENARIO: We increased from 2 cores to 16 cores\")\n",
    "print(\"          but the query got SLOWER! Why?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# BAD: Default partitions (usually 2 on local)\n",
    "print(\"\\n--- BAD: Using default partitions ---\")\n",
    "df_bad = spark.range(5000000) \\\n",
    "    .withColumn(\"group_key\", (col(\"id\") % 1000000)) \\\n",
    "    .withColumn(\"value\", col(\"id\") * 10)\n",
    "\n",
    "print(\"\\nPhysical Plan (look for 'splits'):\")\n",
    "df_bad.groupBy(\"group_key\").agg(count(\"*\")).explain()\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# GOOD: Explicit partitions matching cores\n",
    "print(\"\\n--- GOOD: Explicit partitions = 16 (matching our cores) ---\")\n",
    "df_good = spark.range(0, 5000000, 1, numPartitions=16) \\\n",
    "    .withColumn(\"group_key\", (col(\"id\") % 1000000)) \\\n",
    "    .withColumn(\"value\", col(\"id\") * 10)\n",
    "\n",
    "print(\"\\nPhysical Plan (look for 'splits'):\")\n",
    "df_good.groupBy(\"group_key\").agg(count(\"*\")).explain()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WHAT TO LOOK FOR IN THE PLAN\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "In the Range operator, find 'splits=Some(N)':\n",
    "\n",
    "  BAD:  Range (0, 5000000, step=1, splits=Some(2))\n",
    "        ^\n",
    "        Only 2 partitions! 14 cores sitting idle!\n",
    "\n",
    "  GOOD: Range (0, 5000000, step=1, splits=Some(16))\n",
    "        ^\n",
    "        16 partitions! All cores working!\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "THE RULE:\n",
    "=========\n",
    "    Partitions >= Available Cores\n",
    "\n",
    "    Otherwise, cores sit idle waiting for work!\n",
    "\n",
    "    ┌────────────────────────────────────────────────┐\n",
    "    │  16 Cores with 2 Partitions                    │\n",
    "    │  ┌────┐ ┌────┐ ┌────┐ ... ┌────┐               │\n",
    "    │  │BUSY│ │BUSY│ │IDLE│     │IDLE│  ← 14 idle!   │\n",
    "    │  └────┘ └────┘ └────┘     └────┘               │\n",
    "    └────────────────────────────────────────────────┘\n",
    "\n",
    "    ┌────────────────────────────────────────────────┐\n",
    "    │  16 Cores with 16 Partitions                   │\n",
    "    │  ┌────┐ ┌────┐ ┌────┐ ... ┌────┐               │\n",
    "    │  │BUSY│ │BUSY│ │BUSY│     │BUSY│  ← all work!  │\n",
    "    │  └────┘ └────┘ └────┘     └────┘               │\n",
    "    └────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "HOW TO FIX PARTITION COUNT:\n",
    "===========================\n",
    "\n",
    "1. For spark.range():\n",
    "   spark.range(0, 5000000, 1, numPartitions=16)\n",
    "\n",
    "2. For reading files:\n",
    "   spark.read.option(\"maxPartitionBytes\", \"128MB\").parquet(path)\n",
    "\n",
    "3. For any DataFrame (adds a shuffle!):\n",
    "   df.repartition(16)\n",
    "\n",
    "4. Set default parallelism at session start:\n",
    "   .config(\"spark.default.parallelism\", \"16\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PERFORMANCE COMPARISON: Partition count vs Core count\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 partitions:  7.5 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 partitions: 10.0 seconds\n",
      "\n",
      "→ Results vary - check Spark UI for task distribution\n",
      "\n",
      "======================================================================\n",
      "CHECK SPARK UI - SQL TAB\n",
      "======================================================================\n",
      "\n",
      "Compare the two queries:\n",
      "\n",
      "  7.4a (BAD):  Range shows splits=Some(2)\n",
      "               Stage 0 has only 2 tasks\n",
      "\n",
      "  7.4b (GOOD): Range shows splits=Some(16)\n",
      "               Stage 0 has 16 tasks running in parallel\n",
      "\n",
      "KEY INSIGHT:\n",
      "============\n",
      "More cores WITHOUT more partitions = WORSE performance!\n",
      "\n",
      "The overhead of managing unused resources costs time,\n",
      "while only 2 cores actually do the work.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7.4b: Prove it - Run both and compare times\n",
    "#\n",
    "# Let's actually run both versions and see the difference\n",
    "\n",
    "import time\n",
    "from pyspark.sql.functions import col, count, sum\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERFORMANCE COMPARISON: Partition count vs Core count\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run with 2 partitions (bad)\n",
    "spark.sparkContext.setJobDescription(\"7.4a: BAD - Only 2 partitions\")\n",
    "df_bad = spark.range(5000000) \\\n",
    "    .withColumn(\"group_key\", (col(\"id\") % 1000000)) \\\n",
    "    .withColumn(\"value\", col(\"id\") * 10)\n",
    "\n",
    "start = time.time()\n",
    "df_bad.groupBy(\"group_key\").agg(count(\"*\"), sum(\"value\")).collect()\n",
    "time_bad = time.time() - start\n",
    "print(f\"\\n2 partitions:  {time_bad:.1f} seconds\")\n",
    "\n",
    "# Run with 16 partitions (good)\n",
    "spark.sparkContext.setJobDescription(\"7.4b: GOOD - 16 partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "df_good = spark.range(0, 5000000, 1, numPartitions=16) \\\n",
    "    .withColumn(\"group_key\", (col(\"id\") % 1000000)) \\\n",
    "    .withColumn(\"value\", col(\"id\") * 10)\n",
    "\n",
    "start = time.time()\n",
    "df_good.groupBy(\"group_key\").agg(count(\"*\"), sum(\"value\")).collect()\n",
    "time_good = time.time() - start\n",
    "print(f\"16 partitions: {time_good:.1f} seconds\")\n",
    "\n",
    "# Calculate improvement\n",
    "if time_bad > time_good:\n",
    "    improvement = ((time_bad - time_good) / time_bad) * 100\n",
    "    print(f\"\\n→ {improvement:.0f}% faster with proper parallelism!\")\n",
    "else:\n",
    "    print(f\"\\n→ Results vary - check Spark UI for task distribution\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECK SPARK UI - SQL TAB\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Compare the two queries:\n",
    "\n",
    "  7.4a (BAD):  Range shows splits=Some(2)\n",
    "               Stage 0 has only 2 tasks\n",
    "\n",
    "  7.4b (GOOD): Range shows splits=Some(16)\n",
    "               Stage 0 has 16 tasks running in parallel\n",
    "\n",
    "KEY INSIGHT:\n",
    "============\n",
    "More cores WITHOUT more partitions = WORSE performance!\n",
    "\n",
    "The overhead of managing unused resources costs time,\n",
    "while only 2 cores actually do the work.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8: The Executors Tab\n",
    "\n",
    "The **Executors tab** shows you the health of your cluster.\n",
    "\n",
    "- **Driver**: Your notebook (sends commands)\n",
    "- **Executor(s)**: The workers (do the processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CHECK THE EXECUTORS TAB\n",
      "==================================================\n",
      "\n",
      "1. Go to the EXECUTORS tab\n",
      "\n",
      "2. You should see:\n",
      "   - \"driver\" row - your notebook\n",
      "   - Executor row(s) - the workers\n",
      "\n",
      "3. Important columns:\n",
      "   +---------------------------------------------------------+\n",
      "   | Column         | Good Value | Warning Sign              |\n",
      "   |----------------|------------|---------------------------|\n",
      "   | Disk Used      | 0          | High = spilling to disk   |\n",
      "   | Failed Tasks   | 0          | Any failures = problem    |\n",
      "   | GC Time        | Low        | High = memory pressure    |\n",
      "   +---------------------------------------------------------+\n",
      "\n",
      "KEY INSIGHT:\n",
      "The Executors tab is your \"health dashboard\".\n",
      "Check it when jobs are slow or failing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8.1: Check executor health\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Run a larger job\n",
    "df = spark.range(10_000_000) \\\n",
    "    .withColumn(\"value\", rand() * 1000) \\\n",
    "    .withColumn(\"group\", (col(\"id\") % 50_000))\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"8.1: Large aggregation\")\n",
    "\n",
    "result = df.groupBy(\"group\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"value\").alias(\"total\")\n",
    ")\n",
    "result.show(10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHECK THE EXECUTORS TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. Go to the EXECUTORS tab\n",
    "\n",
    "2. You should see:\n",
    "   - \"driver\" row - your notebook\n",
    "   - Executor row(s) - the workers\n",
    "\n",
    "3. Important columns:\n",
    "   +---------------------------------------------------------+\n",
    "   | Column         | Good Value | Warning Sign              |\n",
    "   |----------------|------------|---------------------------|\n",
    "   | Disk Used      | 0          | High = spilling to disk   |\n",
    "   | Failed Tasks   | 0          | Any failures = problem    |\n",
    "   | GC Time        | Low        | High = memory pressure    |\n",
    "   +---------------------------------------------------------+\n",
    "\n",
    "KEY INSIGHT:\n",
    "The Executors tab is your \"health dashboard\".\n",
    "Check it when jobs are slow or failing.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 9: Event Timeline Colors\n",
    "\n",
    "When you look at a Stage's **Event Timeline**, you'll see colored bars.\n",
    "\n",
    "Each color means something different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1: Understanding Event Timeline colors\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df = spark.range(50000) \\\n",
    "    .withColumn(\"data\", rand())\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"9.1: For Event Timeline\")\n",
    "\n",
    "result = df.groupBy((col(\"id\") % 10).alias(\"bucket\")).agg(\n",
    "    avg(\"data\").alias(\"average\")\n",
    ")\n",
    "result.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORE THE EVENT TIMELINE\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. Jobs tab → Click \"9.1: For Event Timeline\"\n",
    "\n",
    "2. Click on a Stage to see details\n",
    "\n",
    "3. Find \"Event Timeline\" (expand if collapsed)\n",
    "\n",
    "4. Each task is a horizontal bar. The COLORS mean:\n",
    "\n",
    "   +-----------------------------------------+\n",
    "   | Color  | Meaning                        |\n",
    "   |--------|--------------------------------|\n",
    "   | Blue   | Scheduler Delay (waiting)      |\n",
    "   | Red    | Task Deserialization           |\n",
    "   | Orange | Shuffle Read Time              |\n",
    "   | GREEN  | Executor Computing (the work!) |\n",
    "   | Yellow | Shuffle Write Time             |\n",
    "   | Purple | Result Serialization           |\n",
    "   | Cyan   | Getting Result Time            |\n",
    "   +-----------------------------------------+\n",
    "\n",
    "IDEAL: Most of each bar should be GREEN!\n",
    "   - Too much blue? Cluster is overloaded\n",
    "   - Too much orange/yellow? Shuffle is the bottleneck\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 10: Caching\n",
    "\n",
    "If you use the same data multiple times, you can **cache** it.\n",
    "\n",
    "This stores the data in memory so Spark doesn't have to recompute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1: Caching demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: WITHOUT cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|category|        sum(value)|\n",
      "+--------+------------------+\n",
      "|       0| 503731.4222419934|\n",
      "|       7| 501159.7311670624|\n",
      "|       6| 500157.0110107722|\n",
      "|       9|501392.38308998715|\n",
      "|       5| 493392.3598788386|\n",
      "|       1|503723.96725818363|\n",
      "|       3| 499204.3114544312|\n",
      "|       8|499195.94903787115|\n",
      "|       2|500297.06337110884|\n",
      "|       4| 502219.8874024233|\n",
      "+--------+------------------+\n",
      "\n",
      "Time: 1.78 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "Caching the data...\n",
      "\n",
      "Query 2: Populating cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|category|        avg(value)|\n",
      "+--------+------------------+\n",
      "|       0| 50.37314222419934|\n",
      "|       7| 50.11597311670624|\n",
      "|       6| 50.01570110107722|\n",
      "|       9| 50.13923830899871|\n",
      "|       5| 49.33923598788386|\n",
      "|       1| 50.37239672581836|\n",
      "|       3| 49.92043114544312|\n",
      "|       8|49.919594903787115|\n",
      "|       2| 50.02970633711089|\n",
      "|       4| 50.22198874024233|\n",
      "+--------+------------------+\n",
      "\n",
      "Time: 1.69 seconds\n",
      "\n",
      "Query 3: Using cache (should be faster!)\n",
      "+--------+--------+\n",
      "|category|count(1)|\n",
      "+--------+--------+\n",
      "|       0|   10000|\n",
      "|       7|   10000|\n",
      "|       6|   10000|\n",
      "|       9|   10000|\n",
      "|       5|   10000|\n",
      "|       1|   10000|\n",
      "|       3|   10000|\n",
      "|       8|   10000|\n",
      "|       2|   10000|\n",
      "|       4|   10000|\n",
      "+--------+--------+\n",
      "\n",
      "Time: 1.30 seconds\n",
      "\n",
      "==================================================\n",
      "CHECK THE STORAGE TAB NOW!\n",
      "==================================================\n",
      "\n",
      "1. Go to the STORAGE tab\n",
      "\n",
      "2. You should see your cached DataFrame:\n",
      "   - Size in Memory\n",
      "   - Fraction Cached (should be 100%)\n",
      "\n",
      "3. In Jobs tab, compare 10.1a, 10.1b, 10.1c:\n",
      "   - 10.1c might show \"Skipped\" stages (read from cache!)\n",
      "\n",
      "4. In SQL tab, look for \"InMemoryTableScan\"\n",
      "   - This means it read from cache!\n",
      "\n",
      ">>> Run the NEXT cell to unpersist when done viewing <<<\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10.1: Caching demonstration\n",
    "import time\n",
    "from pyspark.sql.functions import avg, rand\n",
    "\n",
    "# Create data\n",
    "df = spark.range(100_000) \\\n",
    "    .withColumn(\"value\", rand() * 100) \\\n",
    "    .withColumn(\"category\", (col(\"id\") % 10))\n",
    "\n",
    "# Query 1: NO cache\n",
    "print(\"Query 1: WITHOUT cache\")\n",
    "spark.sparkContext.setJobDescription(\"10.1a: No cache\")\n",
    "start = time.time()\n",
    "df.groupBy(\"category\").agg(sum(\"value\")).show()\n",
    "print(f\"Time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Now CACHE the DataFrame\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"Caching the data...\")\n",
    "df.cache()\n",
    "\n",
    "# Query 2: Populates the cache\n",
    "print(\"\\nQuery 2: Populating cache\")\n",
    "spark.sparkContext.setJobDescription(\"10.1b: Populate cache\")\n",
    "start = time.time()\n",
    "df.groupBy(\"category\").agg(avg(\"value\")).show()\n",
    "print(f\"Time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Query 3: USES the cache\n",
    "print(\"\\nQuery 3: Using cache (should be faster!)\")\n",
    "spark.sparkContext.setJobDescription(\"10.1c: Using cache\")\n",
    "start = time.time()\n",
    "df.groupBy(\"category\").agg(count(\"*\")).show()\n",
    "print(f\"Time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHECK THE STORAGE TAB NOW!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. Go to the STORAGE tab\n",
    "\n",
    "2. You should see your cached DataFrame:\n",
    "   - Size in Memory\n",
    "   - Fraction Cached (should be 100%)\n",
    "\n",
    "3. In Jobs tab, compare 10.1a, 10.1b, 10.1c:\n",
    "   - 10.1c might show \"Skipped\" stages (read from cache!)\n",
    "\n",
    "4. In SQL tab, look for \"InMemoryTableScan\"\n",
    "   - This means it read from cache!\n",
    "\n",
    ">>> Run the NEXT cell to unpersist when done viewing <<<\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2: Clean up cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared!\n",
      "\n",
      "Check the Storage tab again - it should be empty now.\n"
     ]
    }
   ],
   "source": [
    "# 10.2: Clean up cache\n",
    "#\n",
    "# Run this AFTER you've checked the Storage tab!\n",
    "\n",
    "df.unpersist()\n",
    "print(\"Cache cleared!\")\n",
    "print(\"\\nCheck the Storage tab again - it should be empty now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 11: Joins\n",
    "\n",
    "**Joins** combine data from two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales table:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|sale_id|  store|amount|\n",
      "+-------+-------+------+\n",
      "|      1|Store A|   100|\n",
      "|      2|Store B|   200|\n",
      "|      3|Store A|   150|\n",
      "|      4|Store C|   300|\n",
      "|      5|Store B|   250|\n",
      "+-------+-------+------+\n",
      "\n",
      "Stores table:\n",
      "+-------+-----------+\n",
      "|  store|       city|\n",
      "+-------+-----------+\n",
      "|Store A|   New York|\n",
      "|Store B|Los Angeles|\n",
      "|Store C|    Chicago|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11.1: Create two tables\n",
    "\n",
    "# Sales table\n",
    "sales = spark.createDataFrame([\n",
    "    (1, \"Store A\", 100),\n",
    "    (2, \"Store B\", 200),\n",
    "    (3, \"Store A\", 150),\n",
    "    (4, \"Store C\", 300),\n",
    "    (5, \"Store B\", 250),\n",
    "], [\"sale_id\", \"store\", \"amount\"])\n",
    "\n",
    "# Stores table  \n",
    "stores = spark.createDataFrame([\n",
    "    (\"Store A\", \"New York\"),\n",
    "    (\"Store B\", \"Los Angeles\"),\n",
    "    (\"Store C\", \"Chicago\"),\n",
    "], [\"store\", \"city\"])\n",
    "\n",
    "print(\"Sales table:\")\n",
    "sales.show()\n",
    "\n",
    "print(\"Stores table:\")\n",
    "stores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+-----------+\n",
      "|  store|sale_id|amount|       city|\n",
      "+-------+-------+------+-----------+\n",
      "|Store C|      4|   300|    Chicago|\n",
      "|Store B|      2|   200|Los Angeles|\n",
      "|Store B|      5|   250|Los Angeles|\n",
      "|Store A|      1|   100|   New York|\n",
      "|Store A|      3|   150|   New York|\n",
      "+-------+-------+------+-----------+\n",
      "\n",
      "\n",
      "==================================================\n",
      "CHECK THE SQL TAB\n",
      "==================================================\n",
      "\n",
      "1. Go to SQL tab, find the join query\n",
      "\n",
      "2. Look at the DAG:\n",
      "   - Two \"LocalTableScan\" boxes (reading both tables)\n",
      "   - \"BroadcastHashJoin\" - the join\n",
      "\n",
      "BROADCAST JOIN:\n",
      "The \"stores\" table is tiny, so Spark \"broadcasts\" it -\n",
      "sends the small table to all workers.\n",
      "This avoids shuffling the larger table!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11.2: Join the tables\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"11.2: Join sales with stores\")\n",
    "\n",
    "joined = sales.join(stores, \"store\")\n",
    "joined.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CHECK THE SQL TAB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. Go to SQL tab, find the join query\n",
    "\n",
    "2. Look at the DAG:\n",
    "   - Two \"LocalTableScan\" boxes (reading both tables)\n",
    "   - \"BroadcastHashJoin\" - the join\n",
    "\n",
    "BROADCAST JOIN:\n",
    "The \"stores\" table is tiny, so Spark \"broadcasts\" it -\n",
    "sends the small table to all workers.\n",
    "This avoids shuffling the larger table!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|       city|total_sales|\n",
      "+-----------+-----------+\n",
      "|Los Angeles|        450|\n",
      "|    Chicago|        300|\n",
      "|   New York|        250|\n",
      "+-----------+-----------+\n",
      "\n",
      "\n",
      "==================================================\n",
      "FINAL EXERCISE: ANALYZE THE QUERY\n",
      "==================================================\n",
      "\n",
      "In the SQL tab, find this query. You should see:\n",
      "\n",
      "1. Two scans (reading both tables)\n",
      "2. BroadcastHashJoin (joining)\n",
      "3. HashAggregate (partial sum)\n",
      "4. Exchange (shuffle!)\n",
      "5. HashAggregate (final sum)\n",
      "\n",
      "This combines EVERYTHING from the workshop:\n",
      "- Reading data\n",
      "- Joining tables\n",
      "- Shuffling\n",
      "- Aggregating\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11.3: Join + GroupBy (combining everything!)\n",
    "\n",
    "spark.sparkContext.setJobDescription(\"11.3: Total sales by city\")\n",
    "\n",
    "result = sales \\\n",
    "    .join(stores, \"store\") \\\n",
    "    .groupBy(\"city\") \\\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\"))\n",
    "\n",
    "result.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EXERCISE: ANALYZE THE QUERY\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "In the SQL tab, find this query. You should see:\n",
    "\n",
    "1. Two scans (reading both tables)\n",
    "2. BroadcastHashJoin (joining)\n",
    "3. HashAggregate (partial sum)\n",
    "4. Exchange (shuffle!)\n",
    "5. HashAggregate (final sum)\n",
    "\n",
    "This combines EVERYTHING from the workshop:\n",
    "- Reading data\n",
    "- Joining tables\n",
    "- Shuffling\n",
    "- Aggregating\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Spark is LAZY**: Transformations don't run until you call an action\n",
    "2. **Actions create Jobs**: show(), count(), collect(), write()\n",
    "3. **Jobs contain Stages**: Stages are separated by shuffles\n",
    "4. **Stages contain Tasks**: Tasks run in parallel\n",
    "5. **Shuffles are expensive**: groupBy, join, orderBy move data\n",
    "\n",
    "## Spark UI Tabs\n",
    "\n",
    "| Tab | What to Look For |\n",
    "|-----|------------------|\n",
    "| Jobs | Blue=success, Red=failed, job count |\n",
    "| Stages | Shuffle read/write, skipped stages |\n",
    "| Storage | Cached DataFrames |\n",
    "| Executors | Failed tasks, GC time, disk spill |\n",
    "| SQL | Visual DAG, Exchange = shuffle |\n",
    "\n",
    "## Event Timeline Colors\n",
    "\n",
    "| Color | Meaning |\n",
    "|-------|-------------------------------|\n",
    "| Green | Executor Computing (the work!) |\n",
    "| Blue | Scheduler Delay |\n",
    "| Orange | Shuffle Read |\n",
    "| Yellow | Shuffle Write |\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "# Name your jobs\n",
    "spark.sparkContext.setJobDescription(\"My job name\")\n",
    "\n",
    "# Cache data you reuse\n",
    "df.cache()\n",
    "df.unpersist()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping Spark session...\n",
      "\n",
      "Workshop complete!\n",
      "\n",
      "Key takeaways:\n",
      "1. Spark is LAZY - transformations wait for actions\n",
      "2. Jobs → Stages → Tasks\n",
      "3. Shuffles create new stages (expensive!)\n",
      "4. Use setJobDescription() to label your jobs\n",
      "5. Check Executors tab for cluster health\n"
     ]
    }
   ],
   "source": [
    "# Clean up - run when done\n",
    "print(\"Stopping Spark session...\")\n",
    "spark.stop()\n",
    "print(\"\\nWorkshop complete!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"1. Spark is LAZY - transformations wait for actions\")\n",
    "print(\"2. Jobs → Stages → Tasks\")\n",
    "print(\"3. Shuffles create new stages (expensive!)\")\n",
    "print(\"4. Use setJobDescription() to label your jobs\")\n",
    "print(\"5. Check Executors tab for cluster health\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
